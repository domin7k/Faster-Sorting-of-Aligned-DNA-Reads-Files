\section{Analysis (Version 1.19.2)}

\subsection{Algorithm}
\input{chapters/current_behavior}
\FloatBarrier

\subsection{Time Allocation}
Understanding the resource utilization and the time allocation of the different parts of the sorting process is crucial to be able to optimize its computation time. However, the process has different points of constraint on different machines, as we will see in the following. \\
In general, high time consumption of the SAMtools \texttt{sort} method can be traced to three main blocks.

\subsubsection{Compression} is a part of writing BAM files, as per default compression is applied to all BAM files and even part of the specification. Although compression of BAM files is beneficial in the long term in order to reduce storing costs and transfer speed, it comes with a significant resource overhead. \\
Performing SAMtools \texttt{sort} on a laptop, trough various settings compression and decompression together account for around 95\% of the CPU time. Approximately 80\% are solely required by the \textit{deflate} method that is responsible for the compression.

\subsubsection{IO} can also be a constraint of the sorting process. As the internal mechanisms of SAMtools usually work very fast and are highly parallel, but have to process huge amounts of data, input and output devices can also limit the computation speed.

\subsubsection{Temporary Files} are necessary for SAMtools sort to work as a stream while processing more data than can be held in memory. Unfortunately, writing temporary files is time-consuming. When only looking at the time between reading and decompressing the input and writing and compressing the output, operations involving temporary files lead to the most time consumption. Thus, the amount of temporary files should be minimized. More specific, a BAM record should be written as infrequently as possible. On the other hand, limitations of the Operating System have to be taken into consideration.

\subsection{Compression}
HTSlib is the tool used by SAMtools to perform all file operations. On its README, it claims its only dependency to be \textit{zlib}. zlib is a library used for compression utilizing the DEFLATE algorithm. 

\subsection{IO}


\subsection{Temporary Files}
SAMtools \texttt{sort} has, as mentioned above, a hard coded limit for temporary files. Moreover, this limit is reached very late because of multi level merging. \\
To understand how many temporary files are written, we have to look into the algorithm for merging. The first variable influencing the generation of temporary files is the memory limit. Defaulting to 768MiB, it gets multiplied by the number of threads. The result is the limit up to which BAM records are read in one block. This is also a good approximation for the size of a small temporary file before compression. At least one MiB per thread is enforced to prevent the creation of a huge amount of temporary files. Intuitively, one would think, that the sorting gets faster the more memory can be used. Figure \ref{fig:maxMems} illustrates that this is generally the case, although not in a linear proportion.
\begin{figure}
        \import{figures/}{maxMems.pgf}
    \caption{Execution time of SAMtools \texttt{sort} on a 2.4GB BAM file using default parameters except \texttt{-m} for memory limitation setting. }
    \label{fig:maxMems}
\end{figure}
Moreover, between 400MiB and 12800MiB memory allocation the execution time increases - despite using up to 32 times more memory. To investigate further, we can take a look at the amount of temporary files produced. The input file expands to just a little bigger than the second-highest memory limitation in Figure \ref{fig:maxMems}. Therefore, at the highest setting 25600MiB which equals to 25GiB, no temporary file is produced. On the next highest settings, 1, 2, 4, ... temporary files are produced, as the \texttt{max\_mem} parameter halves to every next highest value. Looking at the amount of temporary files produced, we can also approximate the size of the BAM file in memory. At 400MiB, 32 temporary files are generated as expected. At 200MiB, 65 temporary files are generated. This indicates, that after having processed 12800MiB of data, 200MiB are not enough to keep the remaining data in memory until the final merge into the output file, but 400MiB are. For this reason, the size of the BAM file in memory increases to between 13000MiB and 13200MiB. \\
Now we can see why there are no speed improvements between 400MiB and 12800MiB. In between those settings, exactly the same records are written to the disk, the only difference is the number of files they are spit in. \\
This changes at 200MiB \texttt{max\_mem}. The total of 65 produced temporary files means, that one merge is performed before the final merge and big file is generated. This comes with additional time consumption because the content of the first 32 files has to be read from disk, decompressed, merged, compressed  and written to disk again a second time. \\
At 100MiB 3 big files are generated, at 50MiB 7 and at 25MiB 15. This is also reflected in the total amount of bytes written. While in the parameter settings producing temporary files but not enough of them to be merged to big files 2.4GiB in temporary files are written, this number goes up to 3.7GiB, 4.3Gi, 4.6GiB and 4.8 GiB for 200MiB, 100MiB, 50MiB and 25MiB. Here, the increase in total written bytes for temporary files is not proportional to the amount of merges, as the size of the merged files shrinks with lowering the \texttt{max\_mem} parameter. In Addition, the proportional influence on the total time spend before merging the final result gets lower with the number of performed merges: While writing the first big temporary file costs approximately as much as writing all temporary files before, writing the second one costs only a third of all file writing before, the next one 1/5 then 1/7 and so on.\\
Obviously, the measurements above are unrealistic, as nowadays even Laptops have more memory installed. At the same time, BAM files are usually way bigger than the used sample, which was actually sampled by randomly taking 1\% of a real world BAM file. To get an impression of the impacts of increasing file size, we can look at what changes. \\
Both compression and decompression work in $\mathcal{O}(n)$, ensured by the blockwise compression. The sorting method used is a radix sort, which is also in $\mathcal{O}(n)$. For merging, a heap based approach is chosen, which works in $\mathcal{O}(n \log(k))$. Here, $k$ is the number of sorted list to be merged. Thus, in theory, keeping the same ratio of input size and available memory should produce the same amount of temporary files and in memory files which are the result of sorting and also included in the merging process. Together with all other operations being in linear time, results on little files with little memory should transfer proportional to big files and more memory. This is confirmed by the experiment in Figure \\
\begin{figure}
        \import{figures/}{maxMems.pgf}
    \caption{Execution time of SAMtools \texttt{sort} on a 2.4GB BAM file using default parameters except \texttt{-m} for memory limitation setting. }
    \label{fig:scaleMem}
\end{figure}
However, changing only one of these parameters has different effects. Using SAMtools for example locally installed on a laptop to sort a bigger BAM file can produce many temporary files if memory is limited. If e.g. 8GB are available for SAMtools \texttt{sort}, files bigger than 50GB can not be sorted without merging temporary files. This behavior gets worse if the ratio of the input file to the \texttt{max\_mem} setting grows further.