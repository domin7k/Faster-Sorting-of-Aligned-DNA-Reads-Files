\section{Temporary Files} \label{tempfiles}

\subsection{Overview}

Temporary files act as buffers for \sort, allowing it to process large datasets that exceed the available memory, while supporting streams as input and output. However, writing temporary files is time-consuming, as \sort compresses each temporary file, writes it to disk, and afterward decompresses and reads it again in order to merge the temporary files.

When looking at the time between reading and decompressing the input, and writing and compressing the output, decompressing and compressing temporary files during the sorting process is more time-consuming than the actual sorting of BAM records, which store the alignment information of DNA-Reads in a BAM file. 

Most operating systems limit the number of files that a process is allowed to keep open simultaneously. To address this constraint, \sort employs a merging strategy, reducing the number of open files in the final merge that creates the output files. However, in every merge of temporary files, BAM records, that have been compressed and written to a temporary file before, are compressed and written again. This introduces overhead, as the computation time for decompressing and compressing the content of the merged temporary files again is added to the overall computation time. Thus, we aim at reducing the amount of temporary files \sort utilizes in order to minimize the frequency of \sort compressing and writing each BAM record. In addition, a dynamic merging strategy that adapts to the limitations of the operating system can reduce the number of merges needed for a given number of temporary files, leading to performance improvements.

% 169GiB 0:44:51 [64.6MiB/s]
\subsection{Analysis}
\sort creates a total of 40 temporary files when sorting a 215\,GiB unsorted BAM file, utilizing 16 threads and a total of 32\,GiB memory. Temporary files are BAM files like the output file, but since the main objective for temporary files is processing speed, not disk space, \sort compresses them with compression level 1. This is to maximize the throughput compared to higher compression levels but reduce IO overhead on writing uncompressed BAM files~(\Cref{ioComp}). During writing, nearly all temporary files consume an average of 29.75 seconds from opening the file to closing it. In this time, \sort merges the 16 (one per thread) sorted vectors of BAM records in memory, compresses them, and writes them to the file. However, the 33rd file takes 945.38 seconds. That is 31.7 times the amount of time needed for each temporary file before. 

\sort merges temporary files if it has written a certain number of temporary files. This is to limit the total number of temporary files \sort opens in the final merge. If a program opens too many files concurrently, the operating system kills it (\Cref{limits}). Thus, merging of temporary files is necessary for \sort.
To understand how many temporary files are written and when they are merged, we examine \sort's merging strategy: 

\sort enforces a predefined maximum number of 64 temporary files concurrently stored on disk. Until reaching half of this limit for temporary files, it writes all blocks that are results of sorting the amount of BAM records fitting into memory at once into a single temporary file (a "small file"). Whenever \sort has 32  (half of the limit for temporary files) small temporary files stored on disk concurrently, the next temporary file (a "big file") is a merge of all small temporary files written before together with the next in-memory vectors of sorted BAM records. In summary, on writing every 33rd file (half of the limit for temporary files plus one), \sort performs a merge of 32 small temporary files. This explains the increase in time at writing the 33rd temporary file from the example above: \sort reads every temporary file written before again, merges them and writes their content a second time. 

The number of temporary files on the disk concurrently reaches the limit for temporary files at writing a total of more than the square of half the limit for temporary files (\Cref{limitReaching}). If 33 big temporary files and 31 small temporary files are stored on disk concurrently, \sort merges all small and big temporary files into the next temporary file. For details, refer to \Cref{sorting}. 

The amount of merges depends on the number of temporary files \sort utilizes in total. This is mainly determined by the amount of the memory the user allocates to \sort. The user can configure the memory usage of \sort via the "\texttt{-m}" parameter. Defaulting to 768\,MiB, it gets multiplied by the number of threads. We refer to the result as \texttt{max\_mem}. It is the limit up to which \sort reads BAM records, which contain alignment information to a DNA-Read, into memory, before sorting them in parallel. 
% This is also a good approximation for the size of a small temporary file before compression. 
\sort enforces at least 1\,MiB of memory per thread to prevent the creation of a huge amount of temporary files. In general, sorting is faster the more memory the user allocates to \sort, although not in a linear proportion, as shown in \Cref{fig:maxMems}.
\begin{figure}
        \import{figures/}{maxMems.pgf}
    \caption{Execution time of \sort on a 2.3\,GiB BAM file on a single thread at different memory limitation settings. \points}
    \label{fig:maxMems}
\end{figure}

The execution time of \sort does not decrease on sorting a 2.3\,GiB BAM file using between 400\,MiB and 12800\,MiB of memory.\footnote{Sorting a 2.3\,GiB input file utilizing a memory limitation of 400\,MiB, \sort creates approximately the same amount of temporary files as sorting a 200\,GiB input file utilizing a memory limitation of 32\,GiB.} To investigate further, we examine the amount of temporary files produced. The input file expands to larger than 12800\,MiB, the second-highest memory limitation in \Cref{fig:maxMems}, as the internal representation of BAM records is not compressed in contrast to BAM records in a BAM file. At the highest setting (25600\,MiB $=$ 25\,GiB), \sort utilizes no temporary file. At the next highest settings, it produces 1, 2, 4, â€¦ temporary files, as the memory limitation \texttt{max\_mem} halves to every next highest value. 

Looking at the amount of temporary files generated, it is also possible to approximate the size of the BAM file in memory. At the memory limitation of 400\,MiB, \sort generates 32 temporary files. At 200\,MiB \texttt{max\_mem}, it generates 65 temporary files. This indicates, that after having processed 12800\,MiB ($= \text{400\,MiB} \cdot 32$) of data, 200\,MiB are not enough to keep the remaining data in memory until the final merge into the output file, but 400\,MiB are. \label{blowup}For this reason, the size of the BAM file must increase to between 13000\,MiB and 13200\,MiB in memory, which represents an increase in size by factor 5.52. 

These observations explain the missing speedup between the memory limitations of 400\,MiB and 12800\,MiB. In between these memory limitations, \sort writes exactly the same records to the disk, in exactly the same order. The only difference is the number of temporary files it splits them into. 

This changes at the memory limitation of 200\,MiB \texttt{max\_mem}. The total of 65 produced temporary files means that \sort has to perform a single merge and generate a single big file before the final merge. This comes with additional time consumption because \sort reads the content of the first 32 temporary files from disk, decompresses, merges, compresses, and writes them to disk into a temporary file a second time. 

At the memory limitation of 100\,MiB \sort generates 3 big files, at 50\,MiB 7, and at 25\,MiB 15. This is also reflected in the total amount of bytes written. With the memory limitation settings larger than 200\,MiB \sort utilizes temporary files, but does not merge them into big files. With these memory limitation settings, \sort writes a total of 2.4\,GiB in temporary files. This number goes up to 3.7\,GiB, 4.3\,Gi, 4.6\,GiB and 4.8 GiB for the memory limitations of 200\,MiB, 100\,MiB, 50\,MiB, and 25\,MiB \texttt{max\_mem}. Here, the increase in total written bytes for temporary files is not proportional to the amount of merges, as the size of the merged files shrinks with lowering the memory limitation \texttt{max\_mem}. In addition, the proportional influence, merging of temporary files has on the runtime of \sort lowers with the number of performed merges: While writing the first big temporary file costs approximately the same as writing all temporary files before, writing the second one costs only a third of writing all files before, the next one 1/5 then 1/7 and so on.

The measurements above are unrealistic, as nowadays even Laptops have more memory installed. However, aligned DNA-Read files are usually several times larger than the used sample BAM file, which we sampled by randomly selecting 1\% of BAM records from a real world BAM file. To get an impression of the impacts of increasing the file size, we look at the changes that come with the size increase. 

Both compression and decompression work in $\mathcal{O}(n)$, ensured by the blockwise compression. The sorting algorithm \sort uses is a radix sort, which also is in $\mathcal{O}(n)$. For merging, \sort employs a heap based approach, which operates in $\mathcal{O}(n \log(k))$. Here, $k$ is the number of sorted lists to be merged. Thus, in theory, keeping the same ratio of input size and available memory produces the same amount of temporary files. As all operations (except merging, which also depends on the number of temporary files) are in linear time, results on small files with less memory transfer proportional to large files and more memory. This is confirmed by the experiment shown in Figure \ref{fig:memScaling}.

\begin{figure}[h]
        \import{figures/}{memScaling.pgf}
    \caption{Execution time of \sort on different input sizes. Keeping the ratio from input size to the memory limitation \texttt{max\_mem} constant, the execution time grows linear to both parameters. \points}
    \label{fig:memScaling}
\end{figure}
However, changing only one of these parameters has different effects. Using SAMtools for example installed locally on a laptop for sorting a larger BAM file can produce many temporary files if the laptop's memory is limited. If, e.g., 8\,GB are available for \sort, it cannot process files bigger than 50\,GB without merging temporary files. The file size up to which \sort does not merge temporary files decreases further, if the ratio of the input file to the memory limitation \texttt{max\_mem} grows further.

An important point that should not be exceeded, is reaching 1120 (\Cref{limitReaching}) temporary files. At this point, \sort merges all "big files" (temporary files resulting of a merge of other temporary files) into one single file. This means \sort writes every single BAM record it processed before to disk once more. This occurs approximately at sorting a 1700\,GiB file using 8\,GiB of memory.

In conclusion, writing larger numbers of temporary files, specifically more than 32, leads to merging of temporary files, which is time-consuming. The amount of temporary files \sort utilizes is mainly affected by the ratio of the size of the input file to the amount of available memory.

\subsection{Recommendation}

Since the most time-consuming part of sorting aligned DNA-Read files utilizing \sort is compressing and writing, we aim to minimize the frequency of \sort writing a single BAM record. Consequently, our primary strategy focuses on reducing the number of merge steps performed by \sort during the overall sorting process.

To accomplish this without changing any source code, the user can change the "\texttt{-m}" parameter for per thread memory limitation setting. The more memory the user gives to the process, the less likely \sort needs to merge temporary files. Therefore, we recommend the user to allocate the highest possible memory allowance to \sort within system constraints. To avoid merging of temporary files, the user must set the memory limitation per thread ("\texttt{-m}") to a value larger than $\frac{\text{InputSize} \cdot 6}{31 \cdot \text{\#Threads}} $ for any typical input file compressed with zlib on compression level 6 (the default compression level).\footnote{6 for the blowup of the input file in memory (\Cref{blowup}), 31 for the temporary files written before merging minus 1 to avoid edge cases, \#Threads, as the \texttt{-m} parameter specifies the memory limitation per thread.} However, as the memory limitation the user sets via "\texttt{-m}" is an upper bound only for storing BAM records in memory, SAMtools will most likely exceed it. Thus, the user should not set "\texttt{-m}" to the whole available amount of memory divided by the number of used threads, but keep some memory for SAMtools internal resource allocation. 

However, on devices with limited physical memory, such as laptops, or when working with exceptionally large files (ranging into terabytes of size), allocating enough memory to entirely avoid merging in \sort might not be feasible. Because of this, we recommend enlarging the limit for the number of open temporary files in \sort. At the moment, \sort has a predefined limit of 64 for temporary files stored on disk concurrently. Yet, modern computers are able to keep much more files open without noticeable performance losses. 

\label{limits} On Unix systems, there exist two kinds of limits for the number of open files. The operating system differentiates between \textit{soft limits} and \textit{hard limits}.
A soft limit is a limit set by the user. If a process attempts to open more files than the soft limit, the operating system kills it. On most modern systems, the soft limit is set to 1024 by default. 

The hard limit is the limit up to which the user can increase the soft limit. Its size differs from system to system, but is typically much larger than the hard limit (e.g., 262144 on our testing machine). The hard limit can not be increased by users of the operating system. 

To maximize the limitation for the number of open temporary files in \sort, \sort must obtain the soft limit for open files and calculate the number of files it opens in addition to temporary files.
On a Unix-like operating system, a program can obtain its soft limit using the \texttt{getrlimit} \cite{noauthor_getrlimit2_nodate} system call. \sort only opens temporary files to merge, an output file (or standard output), possibly an index file, and has standard input and standard error open. We use this knowledge to propose a dynamic merging strategy. This strategy recognizes how many files \sort can open concurrently without exceeding the soft limit for open files. Subtracting the maximum number of files (including standard output, standard error, and standard input) \sort opens concurrently to temporary files, the strategy sets an adapted limit for temporary files in \sort. This also enables users to reduce the amount of merges further by increasing the soft limit for open files for \sort. For compatibility reasons, if the system call fails, \sort can still enforce a limit of 64 for temporary files.
 

Notice, that the file size of the input, at which \sort stores as many temporary files as its limit for temporary files allows on disk concurrently, grows quadratic to this limit. At the same time, the file size of the input up to which \sort does not perform a single merge of temporary files grows only half as fast as the maximum number of temporary files.

\subsection{Evaluation}
Increasing the number of allowed temporary files to 1019 ($=1024-5 =$  the default soft limit minus potential other files opened concurrently with temporary files), leads to a 15.5-fold increase in the potential file size of the input file before triggering a merge. With the limit of 1019 for temporary files, \sort performs the first merge of temporary files at the 510th file instead of at the 33rd. Having a limited amount of 8\,GiB of memory, the change to 1019 allowed temporary files raises the file size of the input file, after which the first merge of temporary files occurs, from around 50\,GiB to around 775\,GiB.

Comparing the increased limit for temporary files to the current limit of 64 temporary files, we report a speedup of up to 1.25 at sorting a 2.3\,GiB BAM file with 40\,MiB of memory (\Cref{fig:memSpeedup}).
At the memory limitations between 400\,MiB and 25600\,MiB, we observed identical execution times for both limits for temporary files on sorting a 2.3\,GiB input file. This is due to \sort utilizing between 0 and 32 temporary files. Therefore, it does not merge temporary files with both limitations for temporary files.
Sorting the 2.3\,GiB BAM file with memory limitations between 50\,MiB and 200\,MiB (equivalent to sorting between 1.6\,TiB and 400\,GiB with a memory limitation of 32\,GiB), \sort performs between 1 and 7 merges of temporary files with the limit of 64 for temporary files. In contrast, with the limit for temporary files increased to 1019, sort does not merge temporary files in between these memory limitations. 

However, on the lowest tested memory limitation setting of 25\,MiB for sorting the 2.3\,GiB BAM file (equivalent to sorting a 2.9\,TiB file with a memory limitation of 32\,GiB), the increased limitation for temporary files of 1019 has no speedup compared to the limitation for temporary files of 64. On this memory limitation setting, \sort utilizes a total of 527 temporary files. With the limitation for temporary files set to 1019, \sort merges temporary files the first time at writing the 510th temporary file. Therefore, \sort merges 509 temporary files into the 510th file on the limitation of 1019 for temporary files.

On the limitation of 64 for temporary files, \sort performs 15 merges while writing 527 temporary files. However, as each of these merges only merges 32 temporary files into a single file, only the content of 480 temporary files is written and compressed two times, compared to the content of 509 temporary files at the limitation of 1019 for temporary files. The reason for the difference between the two limits for temporary files is, that \sort includes the in-memory vectors of sorted BAM records, which are the results of parallel sorting, into each merge. Therefore, writing the 510th file, \sort performs less compression and writing operations with the limit of 64 for temporary files, than with the limit of 1019, as it compresses and writes content in the size of 15 temporary files only once. In addition, \sort merges temporary files at the 495th and again at the 528th temporary file if its limit for temporary files is 64. Thus, with this limit for temporary files, at the 510th temporary file, \sort has written 15 temporary files, which are "small files" and not merged into a "big file" until \sort writes the 528th temporary file. The total amount of times, \sort compresses and writes a block of BAM records in size of the available \texttt{max\_mem} into a temporary file is illustrated in \Cref{fig:writes} for both limitations for temporary files.

\begin{figure}
        \import{figures/}{speedupMems.pgf}
    \caption{Speedup after setting the limit for temporary files to 1019. On the memory limitations of more than 200\,MiB \texttt{max\_mem}, for both limitations for the number of temporary files, no merges are performed. At 200\,MiB, 100\,MiB, and 50\,MiB \sort performs 1, 3, and 7 merges with the limit of 64 for temporary files. With the limit of 1019 for temporary files, \sort merges temporary files only at 25\,MiB \texttt{max\_mem}. \points}
    \label{fig:memSpeedup}
% \end{figure}
    \bigskip
% \begin{figure}
        \import{figures/}{writes.pgf}
    \caption{
    Write and compression operations for blocks of BAM records in size of \sort's memory limitation \texttt{max\_mem} on using a limit of 64 and 1019 for temporary files. If 32 files are merged together with one in-memory block, it counts as 33 Operations. Vertical gray lines mark numbers of temporary files produced in the examples in Figures \ref{fig:maxMems} and \ref{fig:memSpeedup}. (E.g., 527 at 25\,MiB \texttt{max\_mem}.)
    }
    \label{fig:writes}
\end{figure}

In summary, increasing the limit for temporary files results in a noticeable speedup if it prevents merging of temporary files. However, with an increased limit for temporary files, each merge of temporary files leads to a higher increase in runtime than a merge with the limitation of 64 for temporary files. If the limit is calculated from the soft limit defined by the operating system, it is maximized and the user can increase it if necessary, preventing merging of temporary files even for sorting terabyte-sized aligned DNA-Read files. 

\subsection{Future Work}
Despite setting the limit for the number of temporary files to 1019, the program initiates merging as early as at writing the 510th temporary file, effectively utilizing only half of the available capacity. Due to merging small temporary files at half of the limit for temporary files, \sort only ever utilizes the full capacity of 1019 temporary files, when the total number of written temporary files exceeds 260,000. While users can potentially eliminate merging by increasing the soft limit, this approach necessitates an unlikely amount of user awareness of this optimization option. To minimize the number of merge steps and reduce the runtime, \sort should ideally utilize the temporary file limit to a greater extent before initiating the merging process. This can be achieved by writing small temporary files not only up to half of the limit for temporary files, but to the limit minus the current amount of big files before merging them. For the first merges, this change would lead to reducing the number of merges by half.