\section{Temporary Files} 

Temporary files are necessary for \sort to work as a stream while processing more data than can be held in memory. Unfortunately, writing temporary files is time-consuming. When only looking at the time between reading and decompressing the input and writing and compressing the output, operations involving temporary files lead to the most time consumption. Thus, the amount of temporary files should be minimized. More specific, a BAM record should be written as infrequently as possible. On the other hand, limitations of the Operating System have to be taken into consideration.

\subsection{Analysis}
Observing the generation of temporary files in \sort, an irregularity stands out. Sorting a 216GB unsorted BAM file utilizing 16 Cores and a total of 32GiB memory, nearly all temporary files take on average 29.75 seconds from opening the file to closing it. In this time, the 16 (one per core) sorted lists in memory are merged and written to the file. Temporary files are compressed like normal BAM files but with GZIP compression level 1. However, the 33rd file takes 945.38 seconds. That is 31.7 times the amount of time needed for the temporary files before. What caused this significant increase? \\
As explained in \ref{sorting}, \sort performs merges of temporary files if a certain number of temporary files is reached. This is to limit the total number of temporary files needed for the final merge. In previous versions of SAMtools where this behavior did not exist, opening to many files at the same time in the final merge caused the program to crash.
To understand how many temporary files are written and when they are merged, one has to look into the algorithm for merging. \\
\sort has, as mentioned above, a hard coded limit for temporary files. Until reaching half of this limit, it writes all blocks that are results of sorting the amount of BAM records fitting into memory at once to a single temporary file. If the limit is reached, the next temporary file is a merge of all small temporary files written before together with the next block of sorted BAM records in memory. In summary, on writing every 33rd file, \sort performs a merge of small temporary files. This explains the increase in time at writing the 33rd temporary file from the example above: \sort reads every temporary file written before again, merges them and writes their content a second time. As the \sort merges temporary files on half of the limit and generates a single file at every merge, the limit is reached later than the square of half the limit. If the limit is reached and 33 big files exist, \sort merges them again together with all small files and the records currently in memory. For details, refer to section \ref{sorting}. \\
The amount of merges depends on the number of temporary files needed in total. This is mainly determined by the amount of the memory the user gives to \sort. The user can set this limit using the "\texttt{-m}" parameter. Defaulting to 768MiB, it gets multiplied by the number of threads. The result is the limit up to which \sort reads BAM records in one block. This is also a good approximation for the size of a small temporary file before compression. At least one MiB per thread is enforced to prevent the creation of a huge amount of temporary files. Intuitively, one would think, that the sorting gets faster the more memory can be used. Figure \ref{fig:maxMems} illustrates that this is generally the case, although not in a linear proportion.
\begin{figure}
        \import{figures/}{maxMems.pgf}
    \caption{Execution time of \sort on a 2.4GB BAM file using default parameters except \texttt{-m} for memory limitation setting. }
    \label{fig:maxMems}
\end{figure}
Moreover, between 400MiB and 12800MiB memory allocation the execution time does not decrease - despite \sort using up to 32 times more memory. To investigate further, one can take a look at the amount of temporary files produced. The input file expands to just a little bigger than the second-highest memory limitation in Figure \ref{fig:maxMems}. Therefore, at the highest setting 25600MiB which equals to 25GiB, no temporary file is produced. At the next highest settings, \sort produces 1, 2, 4, ... temporary files, as the \texttt{max\_mem} parameter halves to every next highest value. Looking at the amount of temporary files generated, it is also possible to approximate the size of the BAM file in memory. At 400MiB, \sort generates 32 temporary files as expected. At 200MiB, 65 temporary files are generated. This indicates, that after having processed 12800MiB of data, 200MiB are not enough to keep the remaining data in memory until the final merge into the output file, but 400MiB are. For this reason, the size of the BAM file must increase to between 13000MiB and 13200MiB in memory. \\
Now it becomes obvious why there are no speed improvements between 400MiB and 12800MiB. In between those settings, \sort writes exactly the same records to the disk, in exactly the same order, the only difference is the number of files they are split into. \\
This changes at 200MiB \texttt{max\_mem}. The total of 65 produced temporary files means, that \sort has to perform a single merge and generate big file before the final merge. This comes with additional time consumption because the content of the first 32 files has to be read from disk, decompressed, merged, compressed  and written to disk again a second time. \\
At 100MiB \sort generates 3 big files, at 50MiB 7 and at 25MiB 15. This is also reflected in the total amount of bytes written. In the parameter settings that produce temporary files but not enough of them to be merged to big files \sort writes a total of 2.4GiB in temporary files. This number goes up to 3.7GiB, 4.3Gi, 4.6GiB and 4.8 GiB for 200MiB, 100MiB, 50MiB and 25MiB. Here, the increase in total written bytes for temporary files is not proportional to the amount of merges, as the size of the merged files shrinks with lowering the \texttt{max\_mem} parameter. In Addition, the proportional influence on the total time spend before merging the final result gets lower with the number of performed merges: While writing the first big temporary file costs approximately as much as writing all temporary files before, writing the second one costs only a third of all file writing before, the next one 1/5 then 1/7 and so on.\\
Obviously, the measurements above are unrealistic, as nowadays even Laptops have more memory installed. At the same time, BAM files are usually way bigger than the used sample, which was actually sampled by randomly taking 1\% of a real world BAM file. To get an impression of the impacts of increasing file size, one can look at what changes. \\
Both compression and decompression work in $\mathcal{O}(n)$, ensured by the blockwise compression. The sorting method used is a radix sort, which is also in $\mathcal{O}(n)$. For merging, a heap based approach is chosen, which works in $\mathcal{O}(n \log(k))$. Here, $k$ is the number of sorted list to be merged. Thus, in theory, keeping the same ratio of input size and available memory should produce the same amount of temporary files and in memory files which are the result of sorting and also included in the merging process. Together with all other operations being in linear time, results on little files with little memory should transfer proportional to big files and more memory. This is confirmed by the experiment shown in Figure \ref{fig:memScaling}.\\
\begin{figure}
        \import{figures/}{memScaling.pgf}
    \caption{Execution time of \sort on different input sizes. Keeping the ratio from input size to \texttt{max\_mem} constant, the execution time grows linear with increasing both parameters.}
    \label{fig:memScaling}
\end{figure}
However, changing only one of these parameters has different effects. Using SAMtools for example locally installed on a laptop to sort a bigger BAM file can produce many temporary files if memory is limited. If e.g. 8GB are available for \sort, files bigger than 50GB can not be sorted without merging temporary files. This behavior gets worse if the ratio of the input file to the \texttt{max\_mem} setting grows further. An important point that should not be exceeded is reaching 1120 intermediate files. At this point, all merged "big files" get merged into one single file, meaning that every single BAM record gets written to disk once more. This occurs approximately at sorting a file of 1700GiB using 8GiB of memory, which is an unlikely use case. \\
In conclusion, writing to bigger amounts of temporary files leads to merging of temporary files, which is time-consuming. This is mainly affected by the ratio of the size of the input file to the amount of available memory.

\subsection{Recommendation}

Since the most time-consuming part of sorting is compressing and writing, the frequency of writing a single BAM record should be minimized. Therefore, \sort should perform as few merges as possible. \\
To archive this without changing any source code, only the "\texttt{-m}" parameter for memory limitation setting can be changed. The more memory the process has, the less likely is a merge of temporary files needed. However, as this limitation is an upper bound only for storing BAM records in memory, SAMtools will most likely exceed it. Thus, "\texttt{-m}" should not be set to the whole available amount of memory divided by the number of used threads, but keep some memory for SAMtools internal resource allocation. \\
Especially on laptops or at working on large files, there is not enough physical memory to avoid merging. Because of this, I recommend enlarging the limit for open temporary files. At the moment, it is set to 64 while modern computers are able to keep much more files open without noticeable performance losses. \\
On Unix systems, there exist two kinds of limits for the number of open files. The operating system differentiates between \textit{soft limits} and \textit{hard limits}.
A soft limit is a limit set by the user. If the soft limit is reached, the process is killed by the operating system. On most modern systems, it is set to 1024 by default. \\
The hard limit is the limit up to which the user can increase the soft limit. Its size differs from system to system, but is typically much larger than the hard limit (e.g. 262144 on the computer used for most of the experiments I present in this work). The hard limit can not be increased. \\
On a Unix operating system, a program can obtain its soft limit using the \texttt{getrlimit} \cite{noauthor_getrlimit2_nodate} system call. Knowing that \sort only opens all the files to merge, an output file (or standard output), possibly an index file and has standard input and standard error open, \texttt{sort} should recognize how many files can be opened and set the limit accordingly. Then, the user can also increase the soft limit, making merging of temporary files obsolete for realistic use cases. For compatibility reasons, if the system call fails, the limit can be kept.
%% but increased???
This is e.g. on Windows machines necessary due to Windows not having a limit for open file handles and thus not supporting \texttt{getrlimit}. \\
Notice, that the necessary file size until the limit is reached grows quadratic to the maximum amount of temporary files \sort allows. On the other hand, the file size up to which \sort does not perform a merge of temporary files grows only half as fast as the maximum number of temporary files.

\subsection{Evaluation}
Increasing the number of allowed temporary files to 1024 while keeping everything else the same leads to a 15.54-fold increase in the potential file size before triggering a merge. \sort then performs the first merge of temporary files at the 513th file instead of at the 33rd. Having a limited amount of 8GiB of memory, the change to 1024 allowed temporary files raises the tipping point, after which the first merge of temporary files occurs, from around 50GiB input size to around 775GiB input size.\\
\begin{figure}
        \import{figures/}{speedupMems.pgf}
    \caption{Speedup after setting the limit for temporary files to 1019. Calculated by dividing the values from Figure \ref{fig:maxMems} by the values of the increased temporary file limit. All other parameters are the same as in Figure \ref{fig:maxMems}.}
    \label{fig:memSpeedup}
\end{figure}
Figure \ref{fig:memSpeedup} shows, that a noticeable speedup occurs only at the lower memory settings but not at the lowest. One can understand this by referring to Figure \ref{fig:writes}.
\begin{figure}
        \import{figures/}{writes.pgf}
    \caption{
    The y-axis shows the amount of compress and write operations for blocks in size of the available memory. This means, if one temporary file is produced, it counts as one write operation. If 32 files are merged together with one in-memory block, it counts as 33 Operations. Vertical gray lines mark numbers of temporary files produced in the example in Figure \ref{fig:maxMems} and \ref{fig:memSpeedup}. (E.g. 527 at 25MiB \texttt{max\_mem}.)
    }
    \label{fig:writes}
\end{figure}
Figure \ref{fig:writes} shows the number of times, a block of BAM records in size of the available \texttt{max\_mem} is compressed and written into a temporary file. Having a limit of 64 temporary files, files are merged relatively often compared to having a limit of 1019 temporary files. Therefore, the graph for the smaller limit has much more steps. On the other hand, changing the limit for temporary files also means, that if temporary files are merged, much more of them are merged at once. The gray, vertical lines mark the number of temporary files created in the example in Figure \ref{fig:maxMems} and \ref{fig:memSpeedup}. If the amount of temporary files is below 33 files, which is true for the seven highest memory settings, the total amount of decompression and write operations (in memory sized blocks) is equal for both limits. After this, the bigger limit gains advantage until the 509th temporary file. Writing the 510th temporary file with a limit of 1019 temporary files means, that \sort rewrites every 509 files before again. For some temporary files, the smaller limit even needs less writes in total. This happens, because after the first merge at the greater limit, the content of every file before is written two times, except the content of the in-memory block of BAM records that is taken into the merge that results in file number 510. However, at the same number of files, considering the smaller limit, also nearly all content of temporary files is written twice. Here, exceptions are again the BAM records being an exclusive part of a big file, as they are taken into a merge with small files but never written into a small file. As this happens much more frequent at the smaller limit, the smaller limit needs less compress and write operations every time the greater limit performs a merge. This changes again at the next merge of the smaller limit. \\
In summary, increasing the limit for temporary files results in a noticeable speedup if it prevents merges. However, if a merge with a greater limit is performed, the impact on the execution time is stronger than with a smaller limit. If the limit is calculated from the soft limit defined by the operating system, it is maximized and the user can increase it if necessary. 

\subsection{Future Work}
Even after setting the limit for temporary files to 1019, \sort merges temporary files the first time at writing the 510th temporary file. Due to merging, \sort reaches the real limit of 1019 files after writing more than 260000 files. While the user can prevent merges by changing the soft limit, knowing about this option is unlikely for an average user. Thus, \sort should use as much of the limit as early as possible to keep the number of merges as low as possible. This can be archived by writing temporary files not only up to half of the limit for temporary files, but to the limit minus the current amount of big files. For the first merges, this change would lead to reducing the number of merges by half.