\section{Temporary Files} 

Temporary files are necessary for SAMtools sort to work as a stream while processing more data than can be held in memory. Unfortunately, writing temporary files is time-consuming. When only looking at the time between reading and decompressing the input and writing and compressing the output, operations involving temporary files lead to the most time consumption. Thus, the amount of temporary files should be minimized. More specific, a BAM record should be written as infrequently as possible. On the other hand, limitations of the Operating System have to be taken into consideration.

\subsection{Analysis}
Observing the generation of temporary files in SAMtools \texttt{sort}, an irregularity stands out. Sorting a 216GB unsorted BAM file utilizing 16 Cores and a total of 32GiB memory, nearly all temporary files take on average 29.75 seconds from opening the file to closing their files. In this time, the 16 sorted lists in memory are merged and written to the file. However, the 33rd file takes 945.38 seconds. That is 31.7 times the amount of time needed for the temporary files before. What caused this significant increase? \\
As explained in \ref{sorting}, SAMtools \texttt{sort} performs merges of temporary files. This is to limit the total number of temporary files needed for the final merge.
To understand how many temporary files are written, we have to look into the algorithm for merging. \\
SAMtools \texttt{sort} has, as mentioned above, a hard coded limit for temporary files. Until half of the limit is reached, all blocks that are results of sorting the amount of BAM records fitting into memory are written to a single temporary file. The next temporary file is a merge of all small temporary files written before and the next block of sorted BAM records in memory. In summary, on writing every 33rd file, a merge of small temporary files is performed. This explains the increase in time at writing the 33rd temporary file above: SAMtools \texttt{sort} reads every temporary file written before again, merges them and writes their content a second time. As the merge is performed on half of the limit and results in a single file, the limit is reached later than the square of half the limit. If the limit is reached and 33 big files exist, they are merged again together with all small files and the in memory records. For details, refer to section \ref{sorting}. \\
The amount of merges depends on the number of temporary files needed in total. This is mainly determined by the amount of the memory the user gives to SAMtools \texttt{sort}. This limit can be set using the "\texttt{-m}" parameter. Defaulting to 768MiB, it gets multiplied by the number of threads. The result is the limit up to which BAM records are read in one block. This is also a good approximation for the size of a small temporary file before compression. At least one MiB per thread is enforced to prevent the creation of a huge amount of temporary files. Intuitively, one would think, that the sorting gets faster the more memory can be used. Figure \ref{fig:maxMems} illustrates that this is generally the case, although not in a linear proportion.
\begin{figure}
        \import{figures/}{maxMems.pgf}
    \caption{Execution time of SAMtools \texttt{sort} on a 2.4GB BAM file using default parameters except \texttt{-m} for memory limitation setting. }
    \label{fig:maxMems}
\end{figure}
Moreover, between 400MiB and 12800MiB memory allocation the execution time increases - despite using up to 32 times more memory. To investigate further, we can take a look at the amount of temporary files produced. The input file expands to just a little bigger than the second-highest memory limitation in Figure \ref{fig:maxMems}. Therefore, at the highest setting 25600MiB which equals to 25GiB, no temporary file is produced. On the next highest settings, 1, 2, 4, ... temporary files are produced, as the \texttt{max\_mem} parameter halves to every next highest value. Looking at the amount of temporary files produced, we can also approximate the size of the BAM file in memory. At 400MiB, 32 temporary files are generated as expected. At 200MiB, 65 temporary files are generated. This indicates, that after having processed 12800MiB of data, 200MiB are not enough to keep the remaining data in memory until the final merge into the output file, but 400MiB are. For this reason, the size of the BAM file in memory increases to between 13000MiB and 13200MiB. \\
Now we can see why there are no speed improvements between 400MiB and 12800MiB. In between those settings, exactly the same records are written to the disk, the only difference is the number of files they are spit in. \\
This changes at 200MiB \texttt{max\_mem}. The total of 65 produced temporary files means, that one merge is performed before the final merge and big file is generated. This comes with additional time consumption because the content of the first 32 files has to be read from disk, decompressed, merged, compressed  and written to disk again a second time. \\
At 100MiB 3 big files are generated, at 50MiB 7 and at 25MiB 15. This is also reflected in the total amount of bytes written. While in the parameter settings producing temporary files but not enough of them to be merged to big files 2.4GiB in temporary files are written, this number goes up to 3.7GiB, 4.3Gi, 4.6GiB and 4.8 GiB for 200MiB, 100MiB, 50MiB and 25MiB. Here, the increase in total written bytes for temporary files is not proportional to the amount of merges, as the size of the merged files shrinks with lowering the \texttt{max\_mem} parameter. In Addition, the proportional influence on the total time spend before merging the final result gets lower with the number of performed merges: While writing the first big temporary file costs approximately as much as writing all temporary files before, writing the second one costs only a third of all file writing before, the next one 1/5 then 1/7 and so on.\\
Obviously, the measurements above are unrealistic, as nowadays even Laptops have more memory installed. At the same time, BAM files are usually way bigger than the used sample, which was actually sampled by randomly taking 1\% of a real world BAM file. To get an impression of the impacts of increasing file size, we can look at what changes. \\
Both compression and decompression work in $\mathcal{O}(n)$, ensured by the blockwise compression. The sorting method used is a radix sort, which is also in $\mathcal{O}(n)$. For merging, a heap based approach is chosen, which works in $\mathcal{O}(n \log(k))$. Here, $k$ is the number of sorted list to be merged. Thus, in theory, keeping the same ratio of input size and available memory should produce the same amount of temporary files and in memory files which are the result of sorting and also included in the merging process. Together with all other operations being in linear time, results on little files with little memory should transfer proportional to big files and more memory. This is confirmed by the experiment shown in Figure \ref{fig:memScaling}.\\
\begin{figure}
        \import{figures/}{memScaling.pgf}
    \caption{Execution time of SAMtools \texttt{sort} on different input sizes. Keeping the ratio from input size to \texttt{max\_mem} constant, the execution time grows linear with increasing both parameters.}
    \label{fig:memScaling}
\end{figure}
However, changing only one of these parameters has different effects. Using SAMtools for example locally installed on a laptop to sort a bigger BAM file can produce many temporary files if memory is limited. If e.g. 8GB are available for SAMtools \texttt{sort}, files bigger than 50GB can not be sorted without merging temporary files. This behavior gets worse if the ratio of the input file to the \texttt{max\_mem} setting grows further. An important point that should not be exceeded is reaching 1120 intermediate files. At this point, all merged "big files" get merged into one single file, meaning that every single BAM record gets written to disk once more. This occurs approximately at sorting a file of 1700GiB using 8GiB of memory, which is an unlikely use case. \\
In conclusion, writing to bigger amounts of temporary files leads to merging of temporary files, which is time-consuming. This is mainly affected by the ratio of the size of the input file to the amount of available memory.

\subsection{Recommendation}

Since the most time-consuming part of sorting is compressing and writing, the frequency of writing a single BAM record should be minimized. Therefore, SAMtools \texttt{sort} should perform as few merges as possible. \\
To archive this without changing any source code, only the "\texttt{-m}" parameter for memory limitation setting can be changed. The more memory the process has, the less likely is a merge of temporary files needed. However, as this limitation is an upper bound only for storing BAM records in memory, SAMtools will most likely exceed it. Thus, "\texttt{-m}" should not be set to the whole available amount of memory divided by the number of used threads, but keep some memory for SAMtools internal resource allocation. \\
Especially on laptops or at working on large files, there is not enough physical memory to avoid merging. Because of this, I recommend enlarging the limit for open temporary files. At the moment, it is set to 64 while modern computers are able to keep much more files open without noticeable performance losses. \\
On Unix systems, there exist two kinds of limits for the number of open files. The operating system differentiates between \textit{soft limits} and \textit{hard limits}.
A soft limit is a limit set by the user. If the soft limit is reached, the process is killed by the operating system. On most modern systems, it is set to 1024 by default. \\
The hard limit is the limit up to which the user can increase the soft limit. Its size differs from system to system, but is typically much larger than the hard limit (e.g. 262144 on the computer used for most of the experiments I present in this work). The hard limit can not be increased. \\
On a Unix operating system, a program can obtain its soft limit using the \texttt{getrlimit} \cite{noauthor_getrlimit2_nodate} system call. Knowing that SAMtools \texttt{sort} only opens all the files to merge, an output file (or standard output), possibly an index file and has standard input and standard error open, \texttt{sort} should recognize how many files can be opened and set the limit accordingly. For compatibility reasons, if the system call fails, the limit can be kept.
%% but increased???
This is e.g. on Windows machines necessary due to Windows not having a limit for open file handles and thus not supporting \texttt{getrlimit}. \\
Notice, that the necessary file size until the limit is reached grows quadratic to the maximum amount of temporary files SAMtools \texttt{sort} allows. On the other hand, the file size up to which SAMtools \texttt{sort} does not perform a merge of temporary files grows only half as fast as the maximum number of temporary files.

\subsection{Evaluation}
Increasing the number of allowed temporary files to 1024 while keeping everything else the same leads to a 15.54-fold increase in the potential file size before triggering a merge. SAMtools \texttt{sort} then performs the first merge of temporary files at the 513th file instead of at the 33rd. Having a limited amount of 8GiB of memory, the change to 1024 allowed temporary files raises the tipping point, after which the first merge of temporary files occurs, from around 50GiB input size to around 775GiB input size. Figure 
\ref{fig:memSpeedup}.\\
\begin{figure}
        \import{figures/}{speedupMems.pgf}
    \caption{Execution time of SAMtools \texttt{sort} on different input sizes. Keeping the ratio from input size to \texttt{max\_mem} constant, the execution time grows linear with increasing both parameters.}
    \label{fig:memSpeedup}
\end{figure}