\section{Temporary Files} \label{tempfiles}

Temporary files act as buffers for \sort, allowing it to process large datasets that exceed available memory while supporting streams as input and output options. However, writing temporary files is time-consuming, as \sort compresses each temporary file, writes it to disk, and then later decompresses and reads it again in order to merge the temporary files. 

When looking at the time between reading and decompressing the input and writing and compressing the output, Decompressing and compressing temporary files during the sorting process are more time-consuming than the actual sorting of BAM records, which store the alignment information of DNA-Reads in a BAM file. 

Furthermore, most operating systems limit the number of files that a process is allowed to keep open simultaneously. To address this constraint, \sort employs a merging strategy, reducing the number of open files in the final merge that creates the output files. However, in every merge of temporary files, BAM records, that have been compressed and written to a temporary file before are compressed and written again. This introduces overhead, as the computation time for compressing the content of the merged temporary files is added to the overall computation time. Thus, \sort should write as few temporary files as possible to reduce the frequency of each BAM record being compressed and written to a temporary file. In addition, a dynamic merging strategy that adapts to the limitations of the operating system can reduce the number of merges needed for a given number of temporary files, leading to performance improvements.

% 169GiB 0:44:51 [64.6MiB/s]
\subsection{Analysis}
\sort creates a total of 40 temporary files when sorting a 216\,GB unsorted BAM file utilizing 16 threads and a total of 32\,GiB memory. Since the main objective for temporary files is processing speed, not disk space, \sort compresses them with compression level 1 archiving maximal troughput compared to higher compression levels but reducing IO overhead from large file sizes. During writing, nearly all temporary files consume an average 29.75 seconds from opening the file to closing it. In this time, \sort merges the 16 (one per thread) sorted vectors of BAM records in memory and writtes them to the file. However, the 33rd file takes 945.38 seconds. That is 31.7 times the amount of time needed for the temporary files before.  

\sort performs merges of temporary files if a certain number of temporary files is reached. This is to limit the total number of temporary files needed for the final merge. In previous versions of SAMtools where this behavior did not exist, opening to many files at the same time in the final merge caused the program to crash.
To understand how many temporary files are written and when they are merged, one has to look into the algorithm for merging. 

 \sort enforces a predefined maximum number of temporary files that can be created during the sorting process. Until reaching half of this limit, it writes all blocks that are results of sorting the amount of BAM records fitting into memory at once to a single temporary file. If the limit is reached, the next temporary file is a merge of all small temporary files written before together with the next block of sorted BAM records in memory. In summary, on writing every 33rd file, \sort performs a merge of small temporary files. This explains the increase in time at writing the 33rd temporary file from the example above: \sort reads every temporary file written before again, merges them and writes their content a second time. As the \sort merges temporary files on half of the limit and generates a single file at every merge, the limit is reached later than the square of half the limit. If the limit is reached and 33 big files exist, \sort merges them again together with all small files and the records currently in memory. For details, refer to section \ref{sorting}. \\
The amount of merges depends on the number of temporary files needed in total. This is mainly determined by the amount of the memory the user gives to \sort. The user can set this limit using the "\texttt{-m}" parameter. Defaulting to 768\,MiB, it gets multiplied by the number of threads. The result is the limit up to which \sort reads BAM records in one block. This is also a good approximation for the size of a small temporary file before compression. At least one MiB per thread is enforced to prevent the creation of a huge amount of temporary files. One might instinctively believe that sorting becomes faster as more memory is utilized. Figure \ref{fig:maxMems} illustrates that this is generally the case, although not in a linear proportion.
\begin{figure}
        \import{figures/}{maxMems.pgf}
    \caption{Execution time of \sort on a 2.4\,GB BAM file using default parameters except \texttt{-m} for memory limitation setting. }
    \label{fig:maxMems}
\end{figure}
Moreover, between 400\,MiB and 12800\,MiB memory allocation the execution time does not decrease - despite \sort using up to 32 times more memory. To investigate further, one can take a look at the amount of temporary files produced. The input file expands to just a little larger than the second-highest memory limitation in Figure \ref{fig:maxMems}. Therefore, at the highest setting 25600\,MiB which equals to 25\,GiB, \sort produces no temporary file. At the next highest settings, it produces 1, 2, 4, ... temporary files, as the \texttt{max\_mem} parameter halves to every next highest value. Looking at the amount of temporary files generated, it is also possible to approximate the size of the BAM file in memory. At 400\,MiB, \sort generates 32 temporary files as expected. At 200\,MiB, it generates 65 temporary files. This indicates, that after having processed 12800\,MiB of data, 200\,MiB are not enough to keep the remaining data in memory until the final merge into the output file, but 400\,MiB are. For this reason, the size of the BAM file must increase to between 13000\,MiB and 13200\,MiB in memory. \\
Now it becomes obvious why there are no speed improvements between 400\,MiB and 12800\,MiB. In between those settings, \sort writes exactly the same records to the disk, in exactly the same order. The only difference is the number of files they are split into. \\
This changes at 200\,MiB \texttt{max\_mem}. The total of 65 produced temporary files means, that \sort has to perform a single merge and generate a single big file before the final merge. This comes with additional time consumption because \sort reads the content of the first 32 files from disk, decompresses, merges, compresses  and writes them to disk a second time. \\
At 100\,MiB \sort generates 3 big files, at 50\,MiB 7 and at 25\,MiB 15. This is also reflected in the total amount of bytes written. In the parameter settings that produce temporary files but not enough of them to be merged to big files \sort writes a total of 2.4\,GiB in temporary files. This number goes up to 3.7\,GiB, 4.3\,Gi, 4.6\,GiB and 4.8 GiB for 200\,MiB, 100\,MiB, 50\,MiB and 25\,MiB. Here, the increase in total written bytes for temporary files is not proportional to the amount of merges, as the size of the merged files shrinks with lowering the \texttt{max\_mem} parameter. In Addition, the proportional influence on the total time spend before merging the final result lowers with the number of performed merges: While writing the first big temporary file costs approximately as much as writing all temporary files before, writing the second one costs only a third of all file writing before, the next one 1/5 then 1/7 and so on.\\
Obviously, the measurements above are unrealistic, as nowadays even Laptops have more memory installed. At the same time, BAM files are usually way bigger than the used sample, which I sampled by randomly taking 1\% of BAM records from a real world BAM file. To get an impression of the impacts of increasing the file size, one can look at the changes that come with the size increase. \\
Both compression and decompression work in $\mathcal{O}(n)$, ensured by the blockwise compression. The sorting method used is a radix sort, which is also in $\mathcal{O}(n)$. For merging, a heap based approach is chosen, which works in $\mathcal{O}(n \log(k))$. Here, $k$ is the number of sorted list to be merged. Thus, in theory, keeping the same ratio of input size and available memory should produce the same amount of temporary files. Together with all other operations being in linear time, results on little files with little memory should transfer proportional to big files and more memory. This is confirmed by the experiment shown in Figure \ref{fig:memScaling}.\\
\begin{figure}
        \import{figures/}{memScaling.pgf}
    \caption{Execution time of \sort on different input sizes. Keeping the ratio from input size to \texttt{max\_mem} constant, the execution time grows linear with increasing both parameters.}
    \label{fig:memScaling}
\end{figure}
However, changing only one of these parameters has different effects. Using SAMtools for example locally installed on a laptop to sort a larger BAM file can produce many temporary files if memory is limited. If e.g. 8\,GB are available for \sort, it cannot process files bigger than 50\,GB without merging temporary files. This behavior gets worse if the ratio of the input file to the \texttt{max\_mem} setting grows further. An important point that should not be exceeded is reaching 1120 temporary files. At this point, \sort merges all "big files" into one single file. This means \sort writes every single BAM record it processed before to disk once more. This occurs approximately at sorting a 1700\,GiB file using 8\,GiB of memory, which is an unlikely use case. \\
In conclusion, writing larger amounts of temporary files leads to merging of temporary files, which is time-consuming. This is mainly affected by the ratio of the size of the input file to the amount of available memory.

\subsection{Recommendation}

Since the most time-consuming part of sorting is compressing and writing, the frequency of writing a single BAM record should be minimized. Therefore, \sort should perform as few merges as possible. \\
To archive this without changing any source code, the user can only change the "\texttt{-m}" parameter for memory limitation setting. The more memory the user gives to the process, the less likely \sort needs to merge temporary files. Therefore, the user should set this limitation as high as possible. However, as the memory limitation the user sets via "\texttt{-m}" is an upper bound only for storing BAM records in memory, SAMtools will most likely exceed it. Thus, the user should not set "\texttt{-m}" to the whole available amount of memory divided by the number of used threads, but keep some memory for SAMtools internal resource allocation. \\
Especially on laptops or for working on large files, the computing device provides not enough physical memory to avoid merging. Because of this, I recommend enlarging the limit for open temporary files. At the moment, it is set to 64 while modern computers are able to keep much more files open without noticeable performance losses. \\
On Unix systems, there exist two kinds of limits for the number of open files. The operating system differentiates between \textit{soft limits} and \textit{hard limits}.
A soft limit is a limit set by the user. If a process reaches the soft limit, the operating system kills it. On most modern systems, the soft limit is set to 1024 by default. \\
The hard limit is the limit up to which the user can increase the soft limit. Its size differs from system to system, but is typically much larger than the hard limit (e.g. 262144 on the computer I used for most of the experiments I present in this work). The hard limit can not be increased. \\
On a Unix operating system, a program can obtain its soft limit using the \texttt{getrlimit} \cite{noauthor_getrlimit2_nodate} system call. Knowing that \sort only opens all the files to merge, an output file (or standard output), possibly an index file and has standard input and standard error open, \texttt{sort} should recognize how many files can be opened and set the limit accordingly. Then, the user can also increase the soft limit, making merging of temporary files obsolete for realistic use cases. For compatibility reasons, if the system call fails, the limit can be kept.
%% but increased???
This is e.g. on Windows machines necessary due to Windows not having a limit for open file handles and thus not supporting \texttt{getrlimit}. \\
Notice, that the necessary file size until the limit is reached grows quadratic to the maximum amount of temporary files \sort allows. On the other hand, the file size up to which \sort does not perform a merge of temporary files grows only half as fast as the maximum number of temporary files.

\subsection{Evaluation}
Increasing the number of allowed temporary files to 1019 ($=1024-5$) while keeping everything else the same leads to a 15.5-fold increase in the potential file size before triggering a merge. \sort then performs the first merge of temporary files at the 513th file instead of at the 33rd. Having a limited amount of 8\,GiB of memory, the change to 1019 allowed temporary files raises the tipping point, after which the first merge of temporary files occurs, from around 50\,GiB input size to around 775\,GiB input size.
\begin{figure}
        \import{figures/}{speedupMems.pgf}
    \caption{Speedup after setting the limit for temporary files to 1019. Calculated by dividing the values from Figure \ref{fig:maxMems} by the values of the increased temporary file limit. All other parameters are the same as in Figure \ref{fig:maxMems}.}
    \label{fig:memSpeedup}
\end{figure}
Figure \ref{fig:memSpeedup} shows, that in the example I presented before, a noticeable speedup occurs only at the lower memory settings but not at the lowest. One can understand this by referring to Figure \ref{fig:writes}.
\begin{figure}
        \import{figures/}{writes.pgf}
    \caption{
    The y-axis shows the amount of compress and write operations for blocks in size of the available memory. This means, if one small temporary file is produced, it counts as one write operation. If 32 files are merged together with one in-memory block, it counts as 33 Operations. Vertical gray lines mark numbers of temporary files produced in the example in Figure \ref{fig:maxMems} and \ref{fig:memSpeedup}. (E.g., 527 at 25\,MiB \texttt{max\_mem}.)
    }
    \label{fig:writes}
\end{figure}
Figure \ref{fig:writes} shows the number of times, a block of BAM records in size of the available \texttt{max\_mem} is compressed and written into a temporary file. Having a limit of 64 temporary files, files are merged relatively often compared to having a limit of 1019 temporary files. Therefore, the graph for the smaller limit has much more steps. On the other hand, changing the limit for temporary files also means, that if temporary files are merged, much more of them are merged at once. The gray, vertical lines mark the number of temporary files \sort creates at the different settings in the example in Figure \ref{fig:maxMems} and \ref{fig:memSpeedup}. If the amount of temporary files is below 33 files, which is true for the seven highest memory settings, the total amount of decompression and write operations (in memory-sized blocks) is equal for both limits. After this, the larger limit gains advantage until the 509th temporary file. Writing the 510th temporary file with a limit of 1019 temporary files means, that \sort rewrites the content of all 509 files it generated before again. For some temporary files, the smaller limit even needs less writes in total. This happens, because after the first merge at the greater limit, the content of every file \sort generated before is written twice, except the content of the in-memory block of BAM records that is taken into the merge that results in file number 510. However, considering the smaller limit, at the same number of files nearly all content of temporary files is written twice as well. Here, exceptions are again the BAM records being an exclusive part of a big file, as they are taken into a merge together with small files but never written into a small file on their own. As this happens much more frequent at the smaller limit, the smaller limit needs less compress and write operations every time the greater limit performs a merge. This changes again at the next merge at the smaller limit. \\
In summary, increasing the limit for temporary files results in a noticeable speedup if it prevents merges. However, if a merge with a greater limit is performed, the impact on the execution time is stronger than with a smaller limit. If the limit is calculated from the soft limit defined by the operating system, it is maximized and the user can increase it if necessary. 

\subsection{Future Work}
Even after setting the limit for temporary files to 1019, \sort merges temporary files the first time at writing the 510th temporary file. Due to merging, \sort reaches the real limit of 1019 files after writing more than 260000 files. While with the proposed changes the user can prevent merges by changing the soft limit, knowing about this option is unlikely for an average user. Thus, \sort should use as much of the limit as early as possible to keep the number of merges as low as possible. This can be archived by writing small temporary files not only up to half of the limit for temporary files, but to the limit minus the current amount of big files before merging them. For the first merges, this change would lead to reducing the number of merges by half.