\section{Compression}
Per specification, compression is a part of writing BAM files. BAM files contain information on the alignment of DNA sequences to a set of reference sequences. The compression method used in BAM files is BGZF (refer to \ref{bgzf}). BGZF internally uses GZIP for compressing small blocks of a file, which BGZF then concatenates. Compression of BAM files reduces storage space and thus storing costs as well as transfer-times of the files over network. However, it comes with a substantial resource overhead. 
COMMENT

\subsection{Analysis}
To measure the impacts of compression and decompression on the computation time that \sort requires, we examine the proportional time consumption of \sort's processing steps. 
Running on 16 threads, with a total of 32\,GiB of memory, it takes \sort 71 minutes and 57 seconds to sort a 215\,GB BAM file. However, it uses only 2 minutes and 35 seconds, (3.6\,\% of the total time), for sorting (merging not included). \\

The profiler VTune~\cite{noauthor_fix_nodate} reveals that much of the remaining time is dedicated to compression.  
Compression and decompression together account for around 95\,\% of the CPU time when performing \sort on a laptop, using various settings. Solely the \textit{deflate} method that is used for the compression and a part of zlib requires approximately 80\,\% of the CPU time. \\
SAMtools has outsourced all file operations to HTSlib. HTSlib depends on zlib for compression and decompression. Compression is done in blocks using the DEFLATE algorithm. Thus, compression and decompression are parallelized: Every time a block is to be compressed or decompressed, HTSlib gives it to a thread pool of workers that compress blocks in parallel.


\subsection{Compression Levels}
Compression levels are the gzip way of trading computation time against space requirements. The user can set them using the "\texttt{-l}" parameter in SAMtools sort. For other SAMtools commands where the "\texttt{-l}" parameter does not exist, the user can still change the compression level of the output via adding \texttt{--output-fmt-option level=1} to the arguments of the command (Put the desired compression level between 0 and 9 instead of \texttt{1}). \\
SAMtools uses two different compression levels. For writing output files, it uses the default compression level of the zlib implementation (compression level 6 for zlib). This can be changed as explained above. For temporary files, SAMtools uses compression level 1. In current SAMtools versions, this can not be changed without changing the source code.
Differences between the compression levels are shown in Figures \ref{fig:compSizes}, \ref{fig:compSpeed}, \ref{fig:bgzfComps}, and \ref{fig:bgzfspeed}.

\subsection{Alternative zlib Implementations}\label{altZlibs}
Being build into the Linux kernel, zlib is seen as the de facto standard of file compressing using the DEFLATE algorithm. The first version of zlib was published in 1995 to be used in the PNG handling library \textit{libpng}. Although still maintained, other libraries have been created that surpass zlib in both compression speed and ratio.\\
For Example, \textit{libdeflate}~\cite{biggers_ebiggerslibdeflate_2024} offers faster compression while archiving a better compression ratio at the same time. Libdeflate archives this through various improvements such as using word access instead of byte access in input reading and match copying, which are parts of the DEFLATE algorithm. Furthermore, it uses a speed-up Huffman decoding process, loads the whole block into a buffer before compressing and utilizes BMI2 instructions on x86\_64 machines if they support them. \\ %% fallen ziemlich vom himmel die sachen. Ich wollte eigentlich nur einen kkeinen Eindruck verschaffen, wieso das schneller ist, aber wahrscheinlich muss ich da entweder tiefer rein, oder das ganz weglassen, oder? -- eher tiefer rein
Support for libdeflate is already built into SAMtools. Moreover, the developers recommend using libdeflate instead of zlib. If HTSlib's configure script finds libdeflate libraries, HTSlib uses them automatically instead of zlib. To decide manually between using zlib and libdeflate, the user can run the HTSlib \texttt{configure} script with the \texttt{--with-libdeflate} resp. \texttt{--without-libdeflate} option. If configured to use libdeflate, HTSlib uses the libdeflate API for compression and decompression. As libdeflate supports 12 compression levels instead of 9 compression levels supported by zlib, SAMtools maps the compression levels as shown in Table \ref{tab:levelMapping}. \\
\begin{table}[]
    \centering
    \begin{tabular}{l|>{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}}c >{\hspace{0.1em}} c >{\hspace{0.1em}} c}
         zlib & \hspace{0.1em} 1 & 2 & 3 & 4 & 5 & \textbf{6} & 7 & 8 & 9 \\
         libdeflate \hspace{0.1em} & \hspace{0.1em} 1 & 2 & 3 & 5 & 6 & \textbf{7} & 8 & 10 & 12 \\
    \end{tabular} \vspace{1em}
    \caption{Mapping between zlib compression levels and libdeflate compression levels in HTSlib. The default level is marked \textbf{bold}.}
    \label{tab:levelMapping}
\end{table}

In addition, the user can choose to use other zlib implementations by using \texttt{LD\_PRELOAD} \cite{myers_intercepting_nodate-1}. \texttt{LD\_PRELOAD} is an environment variable telling the loader to load shared libraries. A shared library is a code object which is not a part of a single program but can be used by multiple programs. Methods and symbols from the shared library are connected to other programs by the linker. If two different definitions for symbols exist, the linker prefers the one from a shared library in \texttt{LD\_PRELOAD} over the one from a shared library that is not in \texttt{LD\_PRELOAD}. \\
For example, HTSlib uses the \texttt{deflate} method of \texttt{libz.so}. However, the user can compile e.g., \textit{zlib-ng} \cite{noauthor_zlib-ngzlib-ng_2024}, which is API compatible to zlib, to a shared object. Then he can specify the path to the compiled shared object in \texttt{LD\_PRELOAD}. As a result, every time HTSlib calls zlib methods, it uses the implementations in zlib-ng. \\
However, this approach is only possible, if the replacement implementation supports the zlib API. Other libraries, which also produce gzip compatible output but use a different API, can not be used for this approach. Partial support is still possible since, if no suitable symbol is found in any shared library from \texttt{LD\_PRELOAD}, the default implementation is used.

\subsection{Testing Non-API-Compatible Compression Libraries Using 7BGZF}
\textit{7BGZF} \cite{yamada_7bgzf_2020} is a tool developed by Taiju Yamada for testing different GZIP compatible compression libraries for BGZF compression in SAMtools. It works by overwriting \texttt{bgzf\_compress}. This is the method HTSlib uses anytime it outputs BGZF compressed files. When running SAMtools with 7BGZF, 7BGZF chooses the library HTSlib uses for compression based on the \texttt{BGZF\_METHOD} environment variable. The user can set the \texttt{BGZF\_METHOD} environment variable to a compression library's name concatenated with a compression level before running a SAMtools command. \\
This approach simplifies testing different compression libraries:
To test 9 libraries, the user only has to do one installation, as 7BGZF's only dependency is \texttt{libc}. Moreover, not all libraries are drop-in for zlib. For igzip for example, 7BGZF has to use a different API. Utilizing 7BGZF abstracts away from the zlib implementations' different APIs, simplifying the switch to another zlib implementation to just a single environment variable adjustment.
\\
On the other hand, using 7BGZF has some disadvantages compared to using \texttt{LD\_PRELOAD}. SAMtools has to link to HTSlib as a shared library rather to linking to the static library, as the method 7BGZF overwrites is a method of HTSlib. To archive this, the user has to change a single line in SAMtools' \texttt{config.mk.in} and change \texttt{@Hsource@HTSLIB = \$(HTSDIR)/libhts.a} to refer to \texttt{libhts.so} instead. \\
Moreover, 7BGZF does not distinguish between different compression levels, which are parameters of the overwritten \texttt{bgzf\_compress} method. Instead, 7BGZF receives the compression level to be used via the \texttt{BGZF\_METHOD} environment variable. Therefore, it applies the same compression level on every written BGZF compressed file, in context of \sort the sorted output files and temporary files. 
This means temporary files, which contain memory-sized chunks of sorted BAM records and are merged into the final output file (refer to \ref{tempfiles}), have the same compression level as output files. Currently, \sort compresses temporary files with compression level 1, regardless of the specified output compression level. Employing a compression level greater than one for the output file will result in increased compression time. Therefore, using files that require \sort to produce temporary files distort comparisons with the default zlib or libdeflate compression. \\
Testing 7BGZF on sorting a BAM file small enough not to produce any temporary files, still gives hints on which libraries to use for faster sorting. For libdeflate and zlib, \sort achieved similar runtimes on average when using 7BGZF compared to plain HTSlib, with variances ranging from 3 to 5 percent. \\

7BGZF claims to support 9 different compression libraries. 
After using the compiling scripts 7BGZF provides, only seven of them are working. Compression by using 7zip and crypto++ fails and defaults to zlib compression. However, the following implementations still work and are tested here:\\
\textit{zlib-ng}~\cite{noauthor_zlib-ngzlib-ng_2024} by Hans Kristian Rosbach is a merge of optimizations of a zlib version by Intel \cite{noauthor_intelzlib_2024} and a zlib fork by Cloudflare \cite{noauthor_cloudflarezlib_2024}. Both merged implementations can be found in old zlib implementation comparisons. The idea behind zlib-ng is to provide a version of zlib witch is more receptive for code changes. Also, due to less need of working for old systems and compilers, many of zlib's workarounds are removed. Mark Adler, the maintainer of zlib also regularly contributes to zlib-ng.\\
\textit{slz} \cite{tarreau_wtarreaulibslz_2024} by Willy Tarreau is a compression library supporting a single compression level only. Slz reduces CPU and RAM usage for web servers by using simplifying parts of the DEFLATE algorithm, namely encoding and matching. These changes also come with performance improvements but a lower compression ratio.\\
\textit{igzip} \cite{tucker_isa-l_2017}, which is a part of the Intelligent Storage Acceleration Library (ISA-L) \cite{noauthor_intelisa-l_2024} by Intel, focuses on compression as fast as possible on cost of the compression ratio. \\
In contrast, Google's \textit{zopfli} \cite{noauthor_googlezopfli_2024} is an algorithm designed by Lode Vandevenne and Jyrki Alakuijala to enable the best possible deflate-compatible compression by finding the best parameters for deflate. However, its implementation is very slow compared to other zlib implementations. \\
\textit{miniz} \cite{noauthor_richgel999miniz_nodate} by Rich Geldreich is another ground up zlib implementation in a single source file. \\
Zlib and libdeflate are also parts of 7BGZF (Section~\ref{altZlibs}).\\

COMMENT

Most of these libraries offer the GZIP-typical compression levels of 1 up to 9 and level 6 as default. Exceptions are libdeflate with compression levels up to 12, igzip with compression levels 1 to 3 defaulting to 1 and slz providing only compression on level 1. For miniz and zopfli, the default compression level is 1. Default compression levels are important to know because of SAMtools using the zlib implementation's default compression level, if no compression level is set via "\texttt{-l}" or "\texttt{--output-fmt-option}". A comparison of features of the tested compression libraries in 7BGZF is shown in Table \ref{tab:libs}.

\begin{table}[]
  \renewcommand{\arraystretch}{1.2}%
    \centering
    \begin{tabularx}{\textwidth}{l|*{7}Y}
         Implementation \hspace{0.5em} & zlib & libdeflate & miniz & igzip & slz & zlib-ng & zopfli  \\
         \hline
         Levels & 1-9 & 1-12 & 1-9 & 1-3 & 1 & 1-9 & -\footnotemark \\
         Decompression & yes & yes & yes & yes & no & yes & no \\
         Drop-In\footnotemark & - & no & yes & no & no & yes\footnotemark & no
    \end{tabularx}
    \vspace{1em}
    \caption{Comparison of features of the compression libraries tested in 7BGZF.}
    \label{tab:libs}
\end{table}
\footnotetext[2]{zopfli does not use compression levels but can specify iterations of the algorithm. Here, the level in experiments is always used as iterations.}
\footnotetext[3]{Drop-In does not infer, that the API provides every symbol of zlib, but that the most important symbols are implemented.}
\footnotetext[4]{The user has to enable the API compatibility in a configuration step before compiling.}

\subsection{7BGZF Results}

On a limited number of threads, specifically fewer than four, faster zlib implementations can archive a speedup of up to 5. To work around 7BGZF's limitation regarding separate compression levels for output and temporary files, I tested it on a file small enough to avoid producing temporary files in \sort. The results can still be used to see which libraries have potential for replacing zlib in HTSlib. I primarily compare them to zlib and libdeflate, since HTSlib already supports these.\\

The best performance could be reached by igzip. On both tested compression levels, it compressed faster than all other compression libraries on their fastest compression level. Using one or two threads, they archived a speedup of up to 5 compared to the default zlib compression. However, igzip's compression rate on both tested compression levels turned out lower than libdeflate on compression level 1 (30\,\% respective 23\,\% of the original size with igzip on compression level 1 respective compression level 3 against 22\,\% of the original size with libdeflate on compression level one, see Figure \ref{fig:bgzfComps}). \\
\begin{figure}[!htb]
        \import{figures/}{compRatiosBGZF.pgf}
    \caption{Compression ratio of different \texttt{zlib} implementations. We reprot the file sizes relative to the uncompressed file. \\
    \texttt{Igzip} on compression level 1 as well as slz and zlib-ng on compression level 1 produce files 50\,\,\% larger than zlib on its default compression level. \texttt{Miniz} on compression level 1 and zlib on compression level 1 produce files which are 2\,0\,\% larger than default zlib. All other settings resulted in differences of up to 10\,\% which are 2\,\% of the uncompressed file.}
    \label{fig:bgzfComps}
\end{figure}
\begin{figure}[!htb]
        \import{figures/}{bgzfSpeedup.pgf}
    \caption{Speedup of \sort using 7BGZF relative to zlib compression level 6 (\sort's default). The libraries are sorted by their single-thread speedup. The numbers in parentheses indicate the compression level. Sorting the 2.3\,GiB default compressed BAM file with 48\,GiB of memory did not produce a temporary file. \\
    Faster compression libraries and lower compression levels, such as igzip, slz, zlib-ng on compression level 1 and libdeflate on compression level 1 are 3.5 to 5 times faster than the default compression if up to two threads are used. Libdeflate and zlib-ng on compression level 6 archive a similar speedup as zlib on compression level 1.}
    \label{fig:bgzfspeed}
\end{figure}
While slz archives a speedup of 4.5 on one and two threads  and is therefore nearly as fast as igzip on level 3, it compressed the tested files to 30\,\% of its uncompressed size, which is more comparable to igzip on compression level 1. \\
Zlib-ng is comparable with libdeflate but offers a wider range. While zlib-ng on compression level 1 archives a speedup of 4 on one or two threads surpassing libdeflate on the same level, it also produces 33\,\% bigger files. However, zlib-ng on compression level 6 produces a 3\,\% smaller file than libdeflate on the same level, but archives a speedup of less than 2 making it noticeably slower than libdeflate on compression level 6. \\
Libdeflate with compression level 6 has a speedup of 2.3 for up to 4 used threads, while producing a 0.5\,\% smaller file than the default zlib compression. This speedup is even larger than the speedup of 2.1 resulting from using zlib on compression level 1. On compression level 1 libdeflate archived a speedup between 3.5 and 4 for up to 4 threads while producing a 6.5\,\% larger file than the default zlib compression. \\
Reducing the compression level to 1, zlib reaches a speedup of approximately 2 for up to 8 threads, while producing a 20\,\% larger file than it does on the default compression level of 6. \\
 Previous experiments showed that zopfli produced around 10\,\% smaller files than the default zlib compression, but took 70 to 90 times longer.   
Miniz provides for each compression level a marginally worse compression ratio than zlib and also has longer processing times, as previous experiments showed. In Figure \ref{fig:bgzfspeed}, Zopfli and miniz are excluded for clarity.\\

Increasing the number of threads gradually reduces the speedup achieved by using faster compression libraries. The speedup of 5 igzip provides on one or two threads diminishes to 4 on 4 threads, to 2.5 on 8 threads and finally to 1.5 on 16 threads, the highest tested number of threads. For all other compression libraries and compression levels, igzip with compression level 1 serves as an upper limit. The speedup of all other libraries stays close to their single-thread speedup until the number of threads, where the speedup of igzip on compression level 1 drops below their single-thread speedup. Then their speedup approaches the speedup of igzip on compression level 1. Therefore, on 16 threads, all alternative zlib implementations on their tested compression levels archive a speedup of approximately 1.5. \\
The speedup compared to their single-threaded execution time of the different compression libraries and compression levels increases slower for the settings archiving a higher single-thread speedup. The speedup of zlib on compression level 6 increases with every higher number of threads, reaching a speedup of 12 on 16 threads. In contrast, faster compression libraries such as igzip show a less substantial increase in speedup. \\
All tested compression libraries and corresponding compression levels that archived a single-thread speedup (compared to zlib on compression level 6) which is as high or higher than libdeflate on compression level 1 reached their highest speedup against their single-thread performance on 8 cores with a speedup of less than 5. When raising the number of threads from 8 to 16, their speedup decreases slightly, as shown in Figure \ref{fig:bgzfSngleCoreSpeedup}. \\
\begin{figure}[!htb]
        \import{figures/}{singleCoreSpeedupBGZF.pgf}
    \caption{Speedup against single-threaded performance of 7BGZF compression libraries and compression levels. Sorting the 2.3\,GiB default compressed BAM file with 48\,GiB of memory did not produce a temporary file. \\
    The speedup of zlib on compression level 6 increases with every higher number of threads. In contrast, faster compression libraries such as igzip show a less substantial increase in speedup. When raising the number of threads from 8 to 16, their speedup decreases slightly.}
    \label{fig:bgzfSngleCoreSpeedup}
\end{figure}
This suggests that beyond a certain number of threads, compression, which is fully parallelized, is no longer the limiting factor. This is supported by the observation that the relative time \sort spends on decompression, sorting and merging together with compression converges to a similar ratio for all compression libraries and tested compression levels. \\
In the following, decompression stands for the time from starting \sort until starting the parallel sorting, sorting for the timespan from starting to ending the sorting and compression the timespan from end of sorting to the end of the \sort process. Having multiple threads available, \sort splits the BAM records in memory and sorts them in parallel. After the sorting, \sort merges the lists of sorted BAM records from the threads, writing each record immediately after merging it (buffered and with compression). Therefore, in the following, compression time includes merging. \\
On 16 threads, all compression libraries on their tested levels used 21\,\% to 31\,\% of their computation time for decompression, 6\,\% to 9\,\% for sorting and 60\,\% to 71\,\% for compression. Here, the faster compression methods use more of their computation time for decompression than slower methods like zlib on compression level 6. However, on a single thread, the faster libraries use in comparison to the slower libraries much more, around 50\,\%, of their time for decompression, while zlib on level 6 only uses 9\,\% of its computation time for decompression. On two threads, the percentage of the computation time the compression methods with a higher single-thread speedup spend on compression decreases further, but grows again with more used threads. Throughout the increasing number of threads, the proportion of compression compared to decompression increases with the faster libraries. In contrast, for the compression methods with less single-thread speedup (compared to zlib on compression level 6), the relative computation time spend on compressing decreases gradually up to around 70\,\% on 16 cores. This is illustrated in Figure \ref{fig:relative7BGZF}.\\
\afterpage{
    \begin{figure}[]
            \import{figures/}{relative7BGZFtimes.pgf}
        \caption{Comparison of the relative runtimes of decompression, sorting, and compression through 7BGZF compression libraries. Decompression: walltime from program startup of \sort until start of the parallel sorting step. Sorting: time spent sorting the DNA-Reads (?). Compression the timespan from end of sorting to the end of the \sort process. Therefore, merging is a part of the compression time as it is done concurrent. While the compression methods with a higher single-thread speedup compared to the default zlib compression on level 6 use a lower percentage of their computation time on compression on a single thread, the usages converge to a similar ratio on 16 threads. Sorting the 2.3\,GiB default compressed BAM file with 48\,GiB of memory did not produce a temporary file.}
        \label{fig:relative7BGZF}
    \end{figure}
    \clearpage
}
Therefore, faster compression seems not to scale as well as decompression. This is most likely due to the merging, which here is part of the compression time. Therefore, for larger numbers of threads, neither compression nor decompression seem to limit the sorting process, but the merging of temporary files. However, this could also shift if \sort produces temporary files. In this case, the merging would for every merged BAM record write and compress it and read and decompress a new one. Although compression and decompression are buffered and not for every read respective written record a new block has to be decompressed respective compressed, overlaps between compression and decompression are likely in this case. In case of in memory sorting, which is analyzed above, compression and decompression at the same time do not exist. 


Besides compression, decompression speed should be taken into account for the full picture. Here, differences between decompressing files compressed by 7BGZF compression libraries are within a range of 15\,\% around the average decompression time for compressed files. This holds for both decompression libraries zlib and libdeflate.
\begin{figure}
        \import{figures/}{decomp.pgf}
    \caption{Decompression speed by SAMtools using zlib compared to using libdeflate for decompression. Measured as execution time of SAMtools \texttt{view} with \texttt{-u} flag on a single core, piped to \texttt{/dev/null}. The test file is a 104\,GB uncompressed BAM file, compressed by \sort using the 7BGZF settings shown on the x-axis. HTSlib was build with \texttt{--with-libdeflate} for decompression utilizing libdeflate respective \texttt{--without-libdeflate} for the decompression utilizing zlib.\\
    Decompressing with Libdeflate takes about half the time as with zlib. The trend is for smaller files to decompress faster than larger ones.}
    \label{fig:decomp}
\end{figure}
The fastest decompression of compressed files varies depending on the decompression library used. Nevertheless, there is a trend indicating that files with smaller sizes, thus higher compression rates, are decompressed faster, as shown in Figure \ref{fig:decomp}. 
Additionally, it is evident that libdeflate is substantially faster than zlib for decompression. This still holds for the uncompressed file ("uncomp" in Figure \ref{fig:decomp}). This is due to a checksum calculation. For each read block, HTSlib calculates a crc32 checksum. Looking at SAMtools \texttt{view} in a profiler, the implementation for the crc32 checksum in libdeflate turns out to be about 10 times faster. \\

In conclusion, igzip, slz as well as zlib-ng and libdeflate on compression level 1 are very fast and suitable for temporary files and files to be used soon. On compression level 6, zlib-ng and libdeflate are good default values and provide a trade-off between file size and computation time. Zopfli offers very good compression, but the huge increase in computation time makes using high compression levels of zlib-ng and libdeflate more preferable in most settings for \sort.

\subsection{Recommendation}
To increase compression and decompression speed, the amount of threads \sort sort uses should be increased. This can be done using the "\texttt{-@}" parameter. Threads are also used for parallel sorting of in memory blocks of read BAM records. This further speeds up the process. The optimal number of threads to use depends on the compression settings. If \sort uses the default zlib implementation with the default compression level 6, using up to 16 threads is reasonable. For faster compression settings, using up to 8 cores is optimal. \\
For compression levels, it comes down to what the purpose of the sorted data is when deciding the level to be used. 
If the file should be archived, setting the level to 9 for maximal compression is an option. However, most of the time, sorting is a step in a larger pipeline, and the data is read and processed further soon. \\
To speed this up, I recommend to use compression level 0 ("\texttt{-l 0}" which is equal to "\texttt{-u}") if the data is not written directly to  disc or transferred over network with limited throughput. \\
If the data is written to disk directly or transferred via network, it comes down to the IO conditions which level to choose (see next section). In most cases, compression level 1 ("\texttt{-l 1}") is a good starting point.\\
When it comes to the zlib implementation to use, I recommend libdeflate. While not being the very fastest implementation, it is already supported by SAMtools. This means the stability is much higher as its usage is tested with SAMtools. Also, the faster crc32 implementation leads to performance improvements. At last, installing and using libdeflate is simpler than using \texttt{LD\_PRELOAD} or changing the environment for every user. After installation, the user can use SAMtools exactly as he has done before without libdeflate, but with better performance. \\
Users, concerned about output sizes but prioritizing a low execution time, can consider using igzip on compression level 1 via 7BGZF. Still, they should compile HTSlib with libdeflate support. 

\subsection{Evaluation}
\begin{figure}[t]
        \import{figures/}{speedupComps.pgf}
    \caption{Speedup of \sort after changing compression parameters. Reference is \sort using HTSlib with default zlib compression. Libdeflate is used via the HTSlib integration, igzip via 7BGZF. The input file is a 23.6\,GB unsorted BAM file with default zlib compression and a total of 48\,GiB memory. The output is piped to \texttt{/dev/null} to minimize IO impacts. Still, a temporary files is written to disk. \sort compresses the 23.6\,GB compressed input file to 21,1\,GB using zlib (the reference), 20.5\,GB using libdeflate on level 6, 22.6\,GB using libdeflate on level 1 and 31.5\,GB using igzip on level 1 (in both settings). \\
    For one to four threads \sort uses, the speedup with using faster compression is up to 5. However, with more threads available, the speedup decreases to approximately 1.5 for all compression methods.}
    \label{fig:speedupCompression}
\end{figure}
\begin{figure}[!h]
        \import{figures/}{speedupFinalCompsSingleCore.pgf}
    \caption{Speedup of \sort after changing compression parameters. The reference is the respective single-core performance (strong scaling). The default zlib implementation benefits most from a higher number of threads. The faster the compression speed, the less benefits come from using a higher number of threads. The underlying data is the data from Figure \ref{fig:speedupCompression}.}
    \label{fig:speedupCompression}
\end{figure}
Using igzip on compression level 1 via BGZF together with libdeflate for decompression leads to a speedup of up to 5 for single core sorting. This remains valid even if \sort generates a temporary file due to insufficient memory relative to the input file size. \\
Using the libdeflate integration of HTSlib we archive a speedup of 2 for the default compression level. Lowering the compression level of libdeflate to 1, we archive a speedup of 3.2 on 1 to 4 cores (see Figure \ref{fig:speedupCompression}). \\
Zlib on default compression level, as well as libdeflate on default compression level, profit from utilizing up to 16 cores (see Figure \ref{fig:speedupCompression}). Libdeflate on compression level one and igzip via 7BGZF profit the most from the increase to 8 threads. Still, changing from 8 to 16 used threads, they are  slightly faster (around 4\,\%), contrary to the previous results, where for igzip the execution time increased at changing from 8 to 16 used threads (see Figure \ref{fig:bgzfSngleCoreSpeedup}). \\
In conclusion, to reduce computation time of the compression done by \sort, the user can lower the compression level and choose a different zlib implementation. Libdeflate emerged to provide higher compression with lower computation time in both compression and decompression. Other libraries like igzip offer faster compression than libdeflate, but are currently not supported by HTSlib. Nevertheless, with certain restrictions, they can still be used via 7BGZF.


\subsection{Future Work}
Intel's igzip performs even better than libdeflate. Although this comes with the downside of larger files, implementing igzip support in HTSlib would enable even faster compression for temporary files. E.g., a mapping could be used mapping compression level 1 and 2 to compression level 1 and 3 of igzip and the higher levels to libdeflate levels. \\
For improving 7BGZF, a differentiation between output files and temporary files could be implemented.\\
Also, the reduced speedup at using faster compression libraries on a larger number of threads could be investigated further. E.g., the merging process could be removed to see if it limits the execution time of \sort.