\section{Compression} 
Compression is a part of writing BAM files. BAM files are per specification BGZF compressed. BGZF internally uses gzip for compressing small blocks of a file, which BGZF then concatenates. Although compression of BAM files is beneficial in the long term in order to reduce storing costs and increase transfer speed, it comes with a significant resource overhead. 

\subsection{Analysis}
Running on 16 threads, with a total of 32\,GiB of memory, \sort takes 71 minutes and 57 seconds to sort a 215\,GB BAM file. However, it uses only 2 minutes and 35 seconds, which are 3.6\% of the total time, for sorting (merging not included). The profiler VTune \cite{noauthor_fix_nodate} reveals that much of the remaining time is dedicated to compression. \\ 
Compression and decompression together account for around 95\% of the CPU time when performing SAMtools \texttt{sort} on a laptop, trough various settings. Solely the \textit{deflate} method that is used for the compression and a part of zlib, requires approximately 80\% of the CPU time. \\
SAMtools has outsourced all file operations to HTSlib. HTSlib depends on zlib for compression and decompression. Compression is done in blocks using the DEFLATE algorithm. Thus, compression and decompression are parallelized: Every time a block is to be compressed or decompressed, HTSlib gives it to a thread pool of workers that compress blocks in parallel.


\subsection{Compression Levels}
Compression Levels are the gzip way of trading computation time against space requirements. The user can set them using the "\texttt{-l}" parameter in SAMtools sort. For other SAMtools commands where the "\texttt{-l}" parameter does not exist, the user can still change the compression level of the output via adding \texttt{--output-fmt-option level=1} to the arguments of the command (Put the desired compression level between 0 and 9 instead of \texttt{1}). \\
In general, SAMtools uses two different compression levels. For writing output files, it uses the default compression level of the zlib implementation (compression level 6 for zlib). This can be changed as explained above. For temporary files, SAMtools uses compression level 1. In current SAMtools versions, this can not be changed without changing the source code.
Differences between the compression levels are shown in Figures \ref{fig:compSizes}, \ref{fig:compSpeed}, \ref{fig:bgzfComps} and \ref{fig:bgzfspeed}.

\subsection{Alternative zlib Implementations}
Being build into the Linux kernel, zlib is seen as the de facto standard of file compressing using the DEFLATE algorithm. The first version of zlib was published in 1995 to be used in the PNG handling library \textit{libpng}. Although still maintained, other libraries have been created that surpass zlib in both compression speed and ratio.\\
For Example, \textit{libdeflate} \cite{biggers_ebiggerslibdeflate_2024} offers faster compression while archiving a better compression ratio at the same time. Libdeflate archives this through various improvements such as using word access instead of byte access in input reading and match copying, which are parts of the DEFLATE algorithm. Furthermore, it uses a speed-up Huffman decoding process, loads the whole block into a buffer before compressing and utilizes BMI2 instructions on x86\_64 machines if they support them. \\ %% fallen ziemlich vom himmel die sachen. Ich wollte eigentlich nur einen kkeinen Eindruck verschaffen, wieso das schneller ist, aber wahrscheinlich muss ich da entweder tiefer rein, oder das ganz weglassen, oder?
Support for libdeflate is already built into SAMtools. Moreover, the developers recommend using libdeflate instead of zlib. If HTSlib's configure script finds libdeflate libraries, HTSlib uses them automatically instead of zlib. To decide manually between using zlib and libdeflate, the user can run the HTSlib \texttt{configure} script with the \texttt{--with-libdeflate} resp. \texttt{--without-libdeflate} option. If it is configured to use libdeflate, HTSlib uses the libdeflate API for compression and decompression. As libdeflate supports 12 compression levels instead of 9 compression levels supported by zlib, SAMtools maps the compression levels as shown in Table \ref{tab:levelMapping}. \\
\begin{table}[]
    \centering
    \begin{tabular}{l|>{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}} c >{\hspace{0.1em}}c >{\hspace{0.1em}} c >{\hspace{0.1em}} c}
         zlib & \hspace{0.1em} 1 & 2 & 3 & 4 & 5 & \textbf{6} & 7 & 8 & 9 \\
         libdeflate \hspace{0.1em} & \hspace{0.1em} 1 & 2 & 3 & 5 & 6 & \textbf{7} & 8 & 10 & 12 \\
    \end{tabular} \vspace{1em}
    \caption{Mapping between zlib compression levels and libdeflate compression levels in HTSlib. The default level is marked \textbf{bold}.}
    \label{tab:levelMapping}
\end{table}

In addition, the user can choose to use other zlib implementations by using \texttt{LD\_PRELOAD} \cite{myers_intercepting_nodate-1}. \texttt{LD\_PRELOAD} is an environment variable telling the loader to load shared libraries. A shared library is a code object which is not part of a single program but can be used by multiple programs. Methods and symbols from the shared library are connected to other programs by the linker. If two different definitions for symbols exist, the linker prefers the one from a shared library in \texttt{LD\_PRELOAD} over the one from a shared library that is not in \texttt{LD\_PRELOAD}. \\
For example, HTSlib uses the \texttt{deflate} method of \texttt{libz.so}. However, the user can compile e.g. \textit{zlib-ng} \cite{noauthor_zlib-ngzlib-ng_2024}, which is API compatible to zlib, to a shared object. Then he can specify the path to the compiled shared object in \texttt{LD\_PRELOAD}. As a result, every time HTSlib calls zlib methods which are also implemented in zlib-ng, it uses the implementations in zlib-ng of the implementations in zlib. \\
However, this approach is only possible, if the replacement implementation supports the zlib API. Other libraries, which also produce gzip compatible output but use a different API, can not be used for this approach. 

\subsection{7BGZF}
\textit{7BGZF} \cite{yamada_7bgzf_2020} is a tool developed by Taiju Yamada for testing different GZIP compatible compression libraries. It works by overwriting \texttt{bgzf\_compress}. This is the method HTSlib uses for compressing. When running SAMtools with 7BGZF, 7BGZF chooses the library HTSlib uses for compression based on the \texttt{BGZF\_METHOD} environment variable. The user can set the \texttt{BGZF\_METHOD} environment variable to a compression library concatenated with a compression level before running a SAMtools command. This approach has the advantage, that it simplifies testing different compression libraries. 
To test 9 libraries, the user only has to do one installation, as 7BGZF's only dependency is \texttt{libc}. Moreover, not all libraries are drop-in for zlib. For igzip for example, 7BGZF has to use a different API. Utilizing 7BGZF abstracts away from the zlib implementation's different APIs, simplifying the switch to another zlib implementation with just a single environment variable adjustment.
\\
On the other hand, using 7BGZF has some disadvantages to using \texttt{LD\_PRELOAD}. SAMtools has to link to a shared library rather to linking to the static library, as the method 7BGZF overwrites is a method of HTSlib. To archive this, the user has to change a single line in SAMtools' \texttt{config.mk.in} and change \texttt{@Hsource@HTSLIB = \$(HTSDIR)/libhts.a} to refer to \texttt{libhts.so} instead. \\
Moreover, 7BGZF does not distinguish between different compression levels, which are parameters of the overwritten \texttt{bgzf\_compress} method. Instead, 7BGZF receives the compression level to be used as part of the \texttt{BGZF\_METHOD} environment variable. Therefore, it applies the same compression level on every written BGZF compressed file. 
In the context of \sort, this means temporary files have the same compression level as output files, which \sort compresses with compression level 1 per default. This leads to more time consumption for compression if the output file should use a compression level which is not level 1. Therefore, using files that require \sort to produce temporary files distort comparisons with default zlib or libdeflate implementation. \\
Testing 7BGZF on sorting a BAM file small enough not to produce any temporary files, still gives hints on which libraries to use for faster sorting. For libdeflate and zlib, \sort achieved similar runtimes on average when using 7BGZF compared to plain HTSlib, with variances ranging from 3 to 5 percent. \\

7BGZF claims to support 9 different compression libraries. 
After using the compiling scripts 7BGZF provides, only seven of them are working. Compression by using 7zip and crypto++ fails and defaults to zlib compression. However, the following implementations still work and are tested here:\\
Next to zlib and libdeflate (see above) \textit{zlib-ng} \cite{noauthor_zlib-ngzlib-ng_2024} by Hans Kristian Rosbach is a merge of optimizations of a now archived zlib version by Intel \cite{noauthor_intelzlib_2024} and a fork by Cloudflare \cite{noauthor_cloudflarezlib_2024}. Both merged implementations can be found in old zlib implementation comparisons. The idea behind zlib-ng is to provide a version of zlib witch is more receptive for code changes. Also, due to less need of working for very old systems and compilers, many of zlib's workarounds are removed. Mark Adler, the maintainer of zlib also regularly contributes to zlib-ng.\\
Googles \textit{zopfli} \cite{noauthor_googlezopfli_2024} is an algorithm designed by Lode Vandevenne and Jyrki Alakuijala to enable the best possible deflate compatible compression by finding the best parameters for deflate. However, its implementation is very slow compared to other zlib implementations. \\
In contrast, \textit{igzip} \cite{tucker_isa-l_2017}, which is a part of the Intelligent Storage Acceleration Library (ISA-L) \cite{noauthor_intelisa-l_2024} by Intel, focuses on compression as fast as possible on cost of the compression ratio. \\
\textit{miniz} \cite{noauthor_richgel999miniz_nodate} by Rich Geldreich is another ground up zlib implementation being in a single source file. \\
The last working library is \textit{slz} \cite{tarreau_wtarreaulibslz_2024} by Willy Tarreau. It aims at reducing resource usage for web servers by using simpler encoding and matching, which are parts of the DEFLATE algorithm. This changes also come with performance improvements but lower compression ratio.\\

Most of these libraries offer the GZIP-typical compression levels of 0 up to 9 and level 6 as default. Exceptions are libdeflate with compression levels up to 12, igzip with compression levels 0 to 3 defaulting to 1 and slz providing only compression on level 1. For miniz and zopfli, the default compression level is 1. Default compression levels are important to know because of SAMtools using the zlib implementation's default compression level, if no compression level is set via "\texttt{-l}" or "\texttt{--output-fmt-option}". A comparison of features of the compression libraries tested in 7BGZF is shown in Table \ref{tab:libs}.

\begin{table}[]
  \renewcommand{\arraystretch}{1.2}%
    \centering
    \begin{tabularx}{\textwidth}{l|*{7}Y}
         Implementation \hspace{0.5em} & zlib & libdeflate & miniz & igzip & slz & zlib-ng & zopfli  \\
         \hline
         Levels & 1-9 & 1-12 & 1-9 & 1-3 & 1 & 1-9 & -\footnotemark \\
         Decompression & yes & yes & yes & yes & no & yes & no \\
         Drop-In\footnotemark & - & no & yes & no & no & yes & no
    \end{tabularx}
    \vspace{1em}
    \caption{Comparison of features of the compression libraries tested in 7BGZF.}
    \label{tab:libs}
\end{table}
\footnotetext[2]{zopfli does not use compression levels but can specify iterations of the algorithm. Here, the level I show in experiments is always used as iterations.}
\footnotetext{Drop-In does not mean, that the API matches to 100\% but that the most important symbols are implemented}

\subsection{7BGZF Results}

On a limited number of threads, specifically fewer than four, we can archive a speedup of up to 5 using BGZF. To work around 7BGZF's limitation regarding separate compression levels for output and temporary files, I tested it on a file small enough to avoid producing temporary files in \sort. The results can still be used to see which libraries have potential for replacing zlib in HTSlib. I primarily compare them to zlib and libdeflate, since HTSlib already supports these.\\
The best performance could be reached by igzip. Both tested compression levels compressed faster than all compression levels of all the other compression libraries. Using one or two threads, they archived a speedup of up to 5 compared to the default zlib compression. However, the compression rate of both tested compression levels turned out lower than libdeflate on compression level 1 (30\% respective 23\% of the original size for igzip on compression level 1 respective compression level 3 against 22\% of the original size for libdeflate on compression level one, see Figure \ref{fig:bgzfComps}). \\
\begin{figure}[!htb]
        \import{figures/}{compRatiosBGZF.pgf}
    \caption{Comparison of the compression ratio of different zlib implementations. Sizes are relative to the uncompressed file. \\
    igzip on compression level 1 together with slz and zlib-ng on compression level 1 produce files 50\% larger than zlib on default compression level. Miniz on compression level 1 and zlib on compression level 1 produce 20\% larger files than default zlib. All other settings resulted in differences of up to 10\% which are 2\% of the uncompressed file.}
    \label{fig:bgzfComps}
\end{figure}
\begin{figure}[!htb]
        \import{figures/}{bgzfSpeedup.pgf}
    \caption{Speedup of \sort using 7BGZF relative to the zlib compression with compression level 6 which is \sort's default compression method. In the legend, libraries are sorted after their speedup on a single thread. The compression level is noted in brackets. Sorting the 2.3\,GiB default compressed BAM file with 48GiB of memory did not produce a temporary file. \\
    Faster compression libraries and lower compression levels, such as igzip, slz, zlib-ng on compression level 1 and libdeflate on compression level 1 are 3.5 to 5 times faster than the default compression if up to two threads are used. Libdeflate and zlib-ng on compression level 6 archive a similar speedup as zlib on compression level 1.}
    \label{fig:bgzfspeed}
\end{figure}
While slz archives a speedup of 4.5 on one and two threads  and is therefore nearly as fast as igzip on level 3, it compressed the tested files to 30\% of its uncompressed size, which is more comparable to igzip on compression level 1. \\
Zlib-ng is comparable with libdeflate but offers a wider range. While zlib-ng on compression level 1 archives a speedup of 4 for one or two threads surpassing libdeflate on the same level, it also produces 33\% bigger files. However, zlib-ng on compression level 6 produces a 3\% smaller file than libdeflate on the same level, but archives a speedup of less than 2 making it noticeably slower than libdeflate on compression level 6. \\
Libdeflate had a speedup of 2.3 for up to 4 used threads, while producing a 0.5\% smaller file than the default zlib compression. This speedup is even larger than the speedup of 2.1 resulting from using zlib on compression level 1. On compression level 1 libdeflate archived a speedup between 3.5 and 4 for up to 4 threads while producing a 6.5\% larger file than the default zlib compression. \\
Reducing the compression level to 1, zlib reaches a speedup of approximately 2 for up to 8 threads, while producing a 20\% larger file than on the default compression level of 6. \\
Zopfli and miniz are excluded for clarity. Previous experiments showed that zopfli produced around 10\% smaller files than the default zlib compression, but took 70 to 90 times longer.   
Miniz provides for each level a marginally worse compression ratio than zlib and also has longer processing times, as previous experiments showed. \\

Increasing the number of threads gradually reduces the speedup achieved by using faster compression libraries. The speedup of 5 igzip provides on one or two threads diminishes to 4 on 4 threads, to 2.5 on 8 threads and finally to 1.5 on 16 threads, the highest tested number of threads. For all other compression libraries and compression levels, igzip at compression level 1 serves as an upper limit. The speedup of all other libraries stays close to their single-thread speedup until the number of threads, where the speedup of igzip on compression level 1 drops below their single-thread speedup. Then their speedup approaches the speedup of igzip on compression level 1. Therefore, on 16 threads, all alternative zlib implementations on their tested compression levels archive a speedup of approximately 1.5. \\
The speedup to the single-threaded execution time of the different compression libraries and compression levels increases slower for the settings archiving a higher single-thread speedup. The speedup of zlib on compression level 6 increases with every higher number of threads, reaching a speedup of 12 on 16 threads. In contrast, faster compression libraries such as igzip show a less significant increase in speedup. All tested compression libraries with the corresponding compression levels that archived a speedup as high or higher than libdeflate on compression level 1 compared to zlib on compression level 6 on a single thread reached their highest speedup against their single-thread performance on 8 cores with a speedup of less than 5. When raising the number of threads from 8 to 16, their speedup decreases slightly, as shown in Figure \ref{fig:bgzfSngleCoreSpeedup}.
\begin{figure}[!htb]
        \import{figures/}{singleCoreSpeedupBGZF.pgf}
    \caption{Speedup against single-threaded performance of 7BGZF compression libraries and compression levels. Sorting the 2.3\,GiB default compressed BAM file with 48GiB of memory did not produce a temporary file. \\
    The speedup of zlib on compression level 6 increases with every higher number of threads. In contrast, faster compression libraries such as igzip show a less significant increase in speedup. When raising the number of threads from 8 to 16, their speedup decreases slightly.}
    \label{fig:bgzfSngleCoreSpeedup}
\end{figure}
This suggests that beyond a certain number of threads, compression, which is fully parallelized, is no longer the limiting factor. This is supported by the observation that the relative time \sort spends on decompression, sorting and compression converges to a similar ratio for all compression libraries on the tested levels. On 16 threads, all compression libraries on the tested levels used 21\% to 31\% of their computation time for decompression, 6\% to 9\% for sorting and 60\% to 71\% for compression. Here, the faster compression methods use more of their computation time for decompression than slower methods like zlib on compression level 6. On a single thread, the faster libraries use much more, around 50\% of their time for decompression, while zlib on level 6 only uses 9\% of its computation time for decompression. On two threads, the percentage of the computation time the compression methods with a higher single-thread speedup spend on compression decreases further, but grows again with more used threads. In contrast, for the compression methods with less single-thread speedup, the relative computation time spend on compressing decreases gradually up to around 70\% on 16 cores. This is illustrated in Figure \ref{fig:relative7BGZF}.\\
\begin{figure}[t]
        \import{figures/}{relative7BGZFtimes.pgf}
    \caption{}
    \label{fig:relative7BGZF}
\end{figure}

Besides compression, decompression speed should be taken into account for the full picture. Here, differences between decompressing files compressed by 7BGZF compression libraries are in a range of 15\% around the average decompression time of compressed files for both decompression libraries.
\begin{figure}[t]
        \import{figures/}{decomp.pgf}
    \caption{Execution time of SAMtools \texttt{view} with \texttt{-u} flag on a single core, piped to \texttt{/dev/null}. The test file is a 104\,GB uncompressed BAM file, compressed by \sort using the 7BGZF settings shown on the x-axis. HTSlib was build with \texttt{--with-libdeflate} for decompression utilizing libdeflate respective \texttt{--without-libdeflate} for the decompression utilizing zlib.}
    \label{fig:decomp}
\end{figure}
Which compressed file is decompressed the fastest varies between the two decompression libraries. Nevertheless, there is a trend indicating that files with smaller sizes, thus higher compression rates, are decompressed faster, as shown in Figure \ref{fig:decomp}. 
Additionally, it is evident that libdeflate is significantly faster than zlib for decompression. This still holds for the uncompressed file ("uncomp" in Figure \ref{fig:decomp}). This is due to a checksum calculation. For each read block, HTSlib calculates a crc32 checksum. Looking at SAMtools \texttt{view} in a profiler, the implementation for the crc32 checksum in libdeflate turns out to be about 10 times faster. \\

In conclusion, igzip, slz as well as zlib-ng and libdeflate on compression level 1 are very fast and suitable for temporary files and files to be used soon. On compression level 6, zlib-ng and libdeflate are good default values and provide a trade-off between file size and computation time. Zopfli offers very good compression, but the huge increase in computation time makes using high compression levels of zlib-ng and libdeflate more preferable in most settings for \sort.

\subsection{Recommendation}
The most obvious way to increase compression speed is to increase the number of available threads. This can be done using the "\texttt{-@}" parameter. Threads are also used for parallel sorting of in memory blocks of read BAM records. This further speeds up the process. \\
For compression levels, it comes down to what the purpose of the sorted data is when deciding the level to be used. 
If the file should be archived, setting the level to 9 for maximal compression is an option. However, most of the time, sorting is a step in a larger pipeline, and the data is read and processed further soon. \\
To speed this up, I recommend to use compression level 0 ("\texttt{-l 0}" which is equal to "\texttt{-u}") if the data is not written directly to  disc or transferred over network with limited throughput. \\
If the data is written to disk directly or transferred via network, it comes down to the IO conditions which level to choose (see next section). In most cases, compression level 1 ("\texttt{-l 1}") is a good starting point.\\
When it comes to the zlib implementation to use, I recommend libdeflate. While not being the very fastest implementation, it is already supported by SAMtools. This means the stability is much higher as its usage is tested with SAMtools. Also, the faster crc32 implementation leads to performance improvements. At last, installing and using libdeflate is simpler than using \texttt{LD\_PRELOAD} or changing the environment for every user. After installation, the user can use SAMtools exactly as he has done before without libdeflate, but with better performance.

\subsection{Evaluation}
\begin{figure}[t]
        \import{figures/}{speedupComps.pgf}
    \caption{Speedup of \sort after changing compression parameters. Reference is \sort using HTSlib with default zlib compression. Libdeflate is used via the HTSlib integration, igzip via 7BGZF. The input file is a 23.6\,GB unsorted BAM file with default zlib compression. \sort had a total of 48\,GiB memory available. The output is piped to \texttt{/dev/null} to minimize IO impacts. Still, limitations in the writing of temporary files or in reading can distort the impacts of changing only compression properties. \sort compresses the 23.6\,GB compressed input file to 21,1\,GB using zlib (the reference), 20.5\,GB using libdeflate on level 6, 22,6\,GB using libdeflate on level 1 and 31.5\,GB using igzip on level 1 (in both settings). \\
    For one to four threads \sort uses, the speedup with using faster compression is up to 5. However, with more threads available, the speedup decreases to approximately 1.5 for all compression methods.}
    \label{fig:speedupCompression}
\end{figure}
\begin{figure}
        \import{figures/}{speedupFinalCompsSingleCore.pgf}
    \caption{Speedup of \sort after changing compression parameters. The reference is the respective single-core performance (strong scaling). The default zlib implementation benefits most from a higher number of threads. The faster the compression speed, the less benefits come from using a higher number of threads.}
    \label{fig:speedupCompression}
\end{figure}
Reducing the compression level and choosing a faster zlib implementation leads to a speedup of up to 5 for single core sorting. As the compression part in \sort is highly parallel, increasing the number of threads also speeds up the process. This lowers the impact of the compression on the total execution time. Therefore, the speedup lowers with increasing the number of threads to 1.5 for all tested compression settings, as shown in Figure \ref{fig:speedupCompression}. \\
In conclusion, to reduce computation time of the compression done by \sort, the user can lower the compression level or choose a different zlib implementation. Libdeflate emerged to provide higher compression with lower computation time in both compression and decompression. Other libraries like igzip offer faster compression than libdeflate, but are currently not supported by HTSlib. 7BGZF is a tool for testing 7 different zlib implementations for compression in HTSlib.


\subsection{Future Work}
Intel's igzip performs even better than libdeflate. Although this comes with the downside of larger files, implementing igzip support in HTSlib would enable even faster compression for temporary files. E.g. a mapping could be used mapping compression level 1 and 2 to compression level 1 and 3 of igzip and the higher levels to libdeflate levels. \\
For improving 7BGZF, a differentiation between output files and temporary files could be implemented.