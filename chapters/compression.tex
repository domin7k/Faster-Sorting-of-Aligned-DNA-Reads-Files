\section{Compression}\label{compression}
Per specification, compression is a part of writing BAM files. BAM files contain information on the alignment of DNA sequences to a set of reference sequences. The compression method used in BAM files is BGZF (refer to \ref{bgzf}). BGZF internally uses GZIP for compressing small blocks of a file, which BGZF then concatenates. Compression of BAM files reduces storage space and thus storing costs as well as transfer-times of the files over network. However, it comes with a substantial resource overhead. 
COMMENT

\subsection{Analysis}
To measure the impacts of compression and decompression on the computation time that \sort requires, we examine the proportional time consumption of \sort's processing steps. 
Running on 16 threads, with a total of 32\,GiB of memory, we report \sort to require 71 minutes and 57 seconds to sort a 215\,GB BAM file. However, \sort uses only 2 minutes and 35 seconds, (3.6\,\% of the total time), for sorting\footnote{Sorting time calculated by adding up the time spans from dividing the blocks of BAM records in memory into one block per thread and the time when all the sorting threads are finished. The times are retrieved by changing \sort's source code to output the current Unix-Timestamp to standard error before and after sorting.} (merging not included), while it uses the rest of the time for reading, writing, compression, decompression, and merging.

The profiler VTune~\cite{noauthor_fix_nodate} reveals that much of the remaining time is dedicated to compression. To analyze the relative time \sort spends in different methods, we executed VTune's Hotspots analysis program locally on a laptop.
Compression and decompression of the temporary and output BAM files, which contain the alignment information to aligned DNA-Reads, together account for 97\,\% of the CPU time when performing \sort on a laptop with the default compression level. On this compression level, the \textit{deflate} method that is used for the compression and a part of zlib requires approximately 81\,\% of the CPU time, the \textit{inflate} method that is used for decompression approximately 11\,\%, and the calculation of the crc32 checksum, which is part of the compression as well as of the decompression, approximately 5\,\%. Setting the compression level to 0 reduces the relative amount of CPU time required for compression and decompression by 1\,\% to 96\,\%. On this compression level, the deflate method requires approximately 68\,\% of \sort's computation time, inflate 19\,\%, and the checksum calculation 9\,\%.

SAMtools utilizes HTSlib for file operations and compression. HTSlib serves as an API for various high-throughput sequencing data formats, such as BAM files, and  provides functionalities for reading and writing them. HTSlib utilizes the zlib library for compression and decompression of compressed file formats (e.g., BAM and CRAM). A file compressed in the BGZF format, the compression method used in the BAM format, consists of a series of blocks of independently compressed data (each smaller than 64\,KB both compressed and uncompressed). To make optimal use of the available processors, HTSlib compresses these blocks in parallel: SAMtools sequentially passes single BAM records containing alignment information on DNA-Reads to HTSlib. HTSlib buffers these BAM records until the next BAM record does not fit into a 64\,KB block together with the other buffered BAM records. Then HTSlib creates a compression and writing job in a thread pool. The thread pool contains queues for each job kind, in case of \sort compression and decompression jobs, and a counter of the amount of queued jobs. The thread pool never wakes more threads than pending jobs. Threads work through jobs of the same queue until the queue is empty. This leads to threads likely staying at the same job and to some threads running in idle, if less jobs then threads are pending. Since all uncompressed blocks are approximately equal in size, active threads receive a balanced workload distribution during the compression or decompression process.


\subsection{Compression Levels}
SAMtools utilizes HTSlib for compression tasks, which utilizes zlib for compression to the DEFLATE format, which is used in the BGZF compression format applied to BAM files.
In zlib, compression levels offer a configurable balance between computational intensity and resulting file size. The DEFLATE format internally used in all of zlib's output formats is a combination of LZ77 and Huffman codes. LZ77 is a \textit{dictionary-based} compression method, which finds matching sequences in a sliding window and replaces following sequences with a reference to the first appearance of a match. The reduction of the file size depends mainly on the amount of matches found and the length of the matching sequences. As shorter matches occur more frequently, finding them is less computational intensive. zlib hashes for every position in the string it compresses 3 bytes and puts a pointer to them into a chained hash list. If the current position is not a part of a previous match, zlib checks the hash table for substrings starting with the same bytes. Then it iteratively compares the substrings to the current string to find the longest match. The parameters after which match-length zlib stops checking the length of the subsequent matches depend on the compression level. E.g, for compression level 1 zlib compares at most 4 substrings per position, reduces this number to 2 if it finds a match of length 4-7, and stops comparing substrings, if it finds a match of length 8 or longer. For compression level 9, zlib compares at most 4096 substrings per position, reduces this number to 2048 if it finds a match of length 32-255, and stops comparing substrings, if it finds a match of length 256 or longer. For compression levels 4-9, zlib also computes the matches for the next window position before eventually accepting a match. This leads to approximately 6 times higher throughput and an approximately 21\,\% larger resulting file size on using compression level 1 instead of compression level 9 for compressing BGZF files in HTSlib utilizing zlib. We show differences between the compression levels' output sizes in \Cref{fig:compSizes} and differences between the compression levels' throughput in \ref{fig:compSpeed}.

\sort uses two different zlib compression levels for the BGZF compression of temporary and output BAM files. For writing output files, it uses the default compression level of the zlib implementation (compression level 6 for zlib), if the user has not set a specific compression level.\footnote{For instructions to set compression levels in SAMtools, refer to \Cref{methodeComp}.} For temporary files, SAMtools uses compression level 1. In current SAMtools versions, this can not be changed without changing the source code.


\subsection{Alternative zlib Implementations}\label{altZlibs}
Being build into the Linux kernel, zlib is seen as the de facto standard of file compressing using the DEFLATE algorithm. However, other libraries have been created that surpass zlib in both compression throughput and smaller file size of the resulting files.

For Example, \textit{libdeflate}~\cite{biggers_ebiggerslibdeflate_2024} offers faster compression than zlib while archiving a better compression ratio at the same time. Libdeflate archives this through various improvements such as using word access instead of byte access in input reading and match copying, which are parts of the DEFLATE algorithm. Furthermore, it uses a speed-up Huffman decoding process, loads the whole block into a buffer before compressing and utilizes BMI2 instructions on x86\_64 machines if they support them. \\ %% fallen ziemlich vom himmel die sachen. Ich wollte eigentlich nur einen kkeinen Eindruck verschaffen, wieso das schneller ist, aber wahrscheinlich muss ich da entweder tiefer rein, oder das ganz weglassen, oder? -- eher tiefer rein
Support for libdeflate is already built into SAMtools. Moreover, the developers recommend using libdeflate instead of zlib. If HTSlib's configure script finds libdeflate libraries, HTSlib uses them automatically instead of zlib.\footnote{To decide manually between zlib and libdeflate see \Cref{turnLibdeflate}}. Since libdeflate offers 12 compression levels compared to zlib's 9 levels, SAMtools implements a mapping scheme (\Cref{compMapping}) to translate user-specified compression levels when utilizing libdeflate for compression tasks.


In addition, the user can choose to use other zlib implementations by using \texttt{LD\_PRELOAD} \cite{myers_intercepting_nodate-1}. 
The \texttt{LD\_PRELOAD} environment variable instructs the dynamic linker to prioritize specific shared libraries during program execution. Shared libraries are reusable code modules that can be loaded by multiple programs. If two different definitions for methods or variables exist, e.g., one definition in the shared library a program uses by default and one in a library in \texttt{LD\_PRELOAD}, the linker prefers the one from a shared library in \texttt{LD\_PRELOAD} over the one from a shared library that is not in \texttt{LD\_PRELOAD}. 

For example, HTSlib uses the \texttt{deflate} method of \texttt{libz.so}. However, the user can compile e.g., \textit{zlib-ng} \cite{noauthor_zlib-ngzlib-ng_2024}, which is API compatible to zlib, to a shared object. Then he can specify the path to the compiled shared object in \texttt{LD\_PRELOAD}. As a result, every time HTSlib calls zlib methods, it uses the implementations in zlib-ng. 

However, this approach is only possible, if the replacement implementation supports the zlib API.  \texttt{LD\_PRELOAD} also allows for partial compatibility. If the dynamic linker does not find a method or variable within the preloaded libraries,  it uses the implementation from the default shared libraries.

\subsection{7BGZF: Testing Non-API-Compatible Compression Libraries}
\textit{7BGZF} \cite{yamada_7bgzf_2020} is a tool for testing different compression libraries for BGZF compression in SAMtools. It works by overwriting the method HTSlib uses anytime it outputs BGZF compressed files, \texttt{bgzf\_compress}. 
Users can choose the compression library and the compression level of 7BGZF via an environment variable.\footnote{For more information on using 7BGZF, refer to \Cref{7bgzfConfig}.}

This approach simplifies testing different compression libraries:
To test 9 libraries, the user only has to do one installation, as 7BGZF's only dependency is \texttt{libc}. Moreover, not all libraries are drop-in for zlib. For igzip for example, 7BGZF has to use a different API. Utilizing 7BGZF abstracts away from the zlib implementations' different APIs, simplifying the switch to another zlib implementation to just a single environment variable adjustment.
\\
On the other hand, using 7BGZF has some disadvantages compared to using \texttt{LD\_PRELOAD}. SAMtools has to link to HTSlib as a shared library rather to linking to the static library, as the method 7BGZF overwrites is a method of HTSlib. To archive this, the user has to change a single line in SAMtools' \texttt{config.mk.in} and change \texttt{@Hsource@HTSLIB = \$(HTSDIR)/libhts.a} to refer to \texttt{libhts.so} instead. \\
Moreover, 7BGZF does not distinguish between different compression levels, which are parameters of the overwritten \texttt{bgzf\_compress} method. Instead, 7BGZF receives the compression level to be used via the \texttt{BGZF\_METHOD} environment variable. Therefore, it applies the same compression level on every written BGZF compressed file, in context of \sort the sorted output files and temporary files. 
This means temporary files, which contain memory-sized chunks of sorted BAM records and are merged into the final output file (refer to \ref{tempfiles}), have the same compression level as output files. Currently, \sort compresses temporary files with compression level 1, regardless of the specified output compression level. Employing a compression level greater than one for the output file will result in increased compression time. Therefore, using files that require \sort to produce temporary files distort comparisons with the default zlib or libdeflate compression. \\
Testing 7BGZF on sorting a BAM file small enough not to produce any temporary files, still gives hints on which libraries to use for faster sorting. For libdeflate and zlib, \sort achieved similar runtimes on average when using 7BGZF compared to plain HTSlib, with variances ranging from 3 to 5 percent. \\

7BGZF claims to support 9 different compression libraries. 
After using the compiling scripts 7BGZF provides, only seven of them are working. Compression by using 7zip and crypto++ fails and defaults to zlib compression. However, the following implementations still work and are tested here:\\
\textit{zlib-ng}~\cite{noauthor_zlib-ngzlib-ng_2024} by Hans Kristian Rosbach is a merge of optimizations of a zlib version by Intel \cite{noauthor_intelzlib_2024} and a zlib fork by Cloudflare \cite{noauthor_cloudflarezlib_2024}. Both merged implementations can be found in old zlib implementation comparisons. The idea behind zlib-ng is to provide a version of zlib witch is more receptive for code changes. Also, due to less need of working for old systems and compilers, many of zlib's workarounds are removed. Mark Adler, the maintainer of zlib also regularly contributes to zlib-ng.\\
\textit{slz} \cite{tarreau_wtarreaulibslz_2024} by Willy Tarreau is a compression library supporting a single compression level only. Slz reduces CPU and RAM usage for web servers by using simplifying parts of the DEFLATE algorithm, namely encoding and matching. These changes also come with performance improvements but a lower compression ratio.\\
\textit{igzip} \cite{tucker_isa-l_2017}, which is a part of the Intelligent Storage Acceleration Library (ISA-L) \cite{noauthor_intelisa-l_2024} by Intel, focuses on compression as fast as possible on cost of the compression ratio. \\
In contrast, Google's \textit{zopfli} \cite{noauthor_googlezopfli_2024} is an algorithm designed by Lode Vandevenne and Jyrki Alakuijala to enable the best possible deflate-compatible compression by finding the best parameters for deflate. However, its implementation is very slow compared to other zlib implementations. \\
\textit{miniz} \cite{noauthor_richgel999miniz_nodate} by Rich Geldreich is another ground up zlib implementation in a single source file. \\
Zlib and libdeflate are also parts of 7BGZF (Section~\ref{altZlibs}).\\

COMMENT

Most of these libraries offer the GZIP-typical compression levels of 1 up to 9 and level 6 as default. Exceptions are libdeflate with compression levels up to 12, igzip with compression levels 1 to 3 defaulting to 1 and slz providing only compression on level 1. For miniz and zopfli, the default compression level is 1. Default compression levels are important to know because of SAMtools using the zlib implementation's default compression level, if no compression level is set via "\texttt{-l}" or "\texttt{--output-fmt-option}". A comparison of features of the tested compression libraries in 7BGZF is shown in Table \ref{tab:libs}.

\begin{table}[]
  \renewcommand{\arraystretch}{1.2}%
    \centering
    \begin{tabularx}{\textwidth}{l|*{7}Y}
         Implementation \hspace{0.5em} & zlib & libdeflate & miniz & igzip & slz & zlib-ng & zopfli  \\
         \hline
         Levels & 1-9 & 1-12 & 1-9 & 1-3 & 1 & 1-9 & -\footnotemark \\
         Decompression & yes & yes & yes & yes & no & yes & no \\
         Drop-In\footnotemark & - & no & yes & no & no & yes\footnotemark & no
    \end{tabularx}
    \vspace{1em}
    \caption{Comparison of features of the compression libraries tested in 7BGZF.}
    \label{tab:libs}
\end{table}
\footnotetext[2]{zopfli does not use compression levels but can specify iterations of the algorithm. Here, the level in experiments is always used as iterations.}
\footnotetext[3]{Drop-In does not infer, that the API provides every symbol of zlib, but that the most important symbols are implemented.}
\footnotetext[4]{The user has to enable the API compatibility in a configuration step before compiling.}

\subsection{7BGZF Results}

On a limited number of threads, specifically fewer than four, faster zlib implementations can archive a speedup of up to 5. To work around 7BGZF's limitation regarding separate compression levels for output and temporary files, I tested it on a file small enough to avoid producing temporary files in \sort. The results can still be used to see which libraries have potential for replacing zlib in HTSlib. I primarily compare them to zlib and libdeflate, since HTSlib already supports these.\\

The best performance could be reached by igzip. On both tested compression levels, it compressed faster than all other compression libraries on their fastest compression level. Using one or two threads, they archived a speedup of up to 5 compared to the default zlib compression. However, igzip's compression rate on both tested compression levels turned out lower than libdeflate on compression level 1 (30\,\% respective 23\,\% of the original size with igzip on compression level 1 respective compression level 3 against 22\,\% of the original size with libdeflate on compression level one, see Figure \ref{fig:bgzfComps}). \\
\begin{figure}[!htb]
        \import{figures/}{compRatiosBGZF.pgf}
    \caption{Compression ratio of different \texttt{zlib} implementations. We reprot the file sizes relative to the uncompressed file. \\
    \texttt{Igzip} on compression level 1 as well as slz and zlib-ng on compression level 1 produce files 50\,\,\% larger than zlib on its default compression level. \texttt{Miniz} on compression level 1 and zlib on compression level 1 produce files which are 2\,0\,\% larger than default zlib. All other settings resulted in differences of up to 10\,\% which are 2\,\% of the uncompressed file.}
    \label{fig:bgzfComps}
\end{figure}
\begin{figure}[!htb]
        \import{figures/}{bgzfSpeedup.pgf}
    \caption{Speedup of \sort using 7BGZF relative to zlib compression level 6 (\sort's default). The libraries are sorted by their single-thread speedup. The numbers in parentheses indicate the compression level. Sorting the 2.3\,GiB default compressed BAM file with 48\,GiB of memory did not produce a temporary file. \\
    Faster compression libraries and lower compression levels, such as igzip, slz, zlib-ng on compression level 1 and libdeflate on compression level 1 are 3.5 to 5 times faster than the default compression if up to two threads are used. Libdeflate and zlib-ng on compression level 6 archive a similar speedup as zlib on compression level 1.}
    \label{fig:bgzfspeed}
\end{figure}
While slz archives a speedup of 4.5 on one and two threads  and is therefore nearly as fast as igzip on level 3, it compressed the tested files to 30\,\% of its uncompressed size, which is more comparable to igzip on compression level 1. \\
Zlib-ng is comparable with libdeflate but offers a wider range. While zlib-ng on compression level 1 archives a speedup of 4 on one or two threads surpassing libdeflate on the same level, it also produces 33\,\% bigger files. However, zlib-ng on compression level 6 produces a 3\,\% smaller file than libdeflate on the same level, but archives a speedup of less than 2 making it noticeably slower than libdeflate on compression level 6. \\
Libdeflate with compression level 6 has a speedup of 2.3 for up to 4 used threads, while producing a 0.5\,\% smaller file than the default zlib compression. This speedup is even larger than the speedup of 2.1 resulting from using zlib on compression level 1. On compression level 1 libdeflate archived a speedup between 3.5 and 4 for up to 4 threads while producing a 6.5\,\% larger file than the default zlib compression. \\
Reducing the compression level to 1, zlib reaches a speedup of approximately 2 for up to 8 threads, while producing a 20\,\% larger file than it does on the default compression level of 6. \\
 Previous experiments showed that zopfli produced around 10\,\% smaller files than the default zlib compression, but took 70 to 90 times longer.   
Miniz provides for each compression level a marginally worse compression ratio than zlib and also has longer processing times, as previous experiments showed. In Figure \ref{fig:bgzfspeed}, Zopfli and miniz are excluded for clarity.\\

Increasing the number of threads gradually reduces the speedup achieved by using faster compression libraries. The speedup of 5 igzip provides on one or two threads diminishes to 4 on 4 threads, to 2.5 on 8 threads and finally to 1.5 on 16 threads, the highest tested number of threads. For all other compression libraries and compression levels, igzip with compression level 1 serves as an upper limit. The speedup of all other libraries stays close to their single-thread speedup until the number of threads, where the speedup of igzip on compression level 1 drops below their single-thread speedup. Then their speedup approaches the speedup of igzip on compression level 1. Therefore, on 16 threads, all alternative zlib implementations on their tested compression levels archive a speedup of approximately 1.5. \\
The speedup compared to their single-threaded execution time of the different compression libraries and compression levels increases slower for the settings archiving a higher single-thread speedup. The speedup of zlib on compression level 6 increases with every higher number of threads, reaching a speedup of 12 on 16 threads. In contrast, faster compression libraries such as igzip show a less substantial increase in speedup. \\
All tested compression libraries and corresponding compression levels that archived a single-thread speedup (compared to zlib on compression level 6) which is as high or higher than libdeflate on compression level 1 reached their highest speedup against their single-thread performance on 8 cores with a speedup of less than 5. When raising the number of threads from 8 to 16, their speedup decreases slightly, as shown in Figure \ref{fig:bgzfSngleCoreSpeedup}. \\
\begin{figure}[!htb]
        \import{figures/}{singleCoreSpeedupBGZF.pgf}
    \caption{Speedup against single-threaded performance of 7BGZF compression libraries and compression levels. Sorting the 2.3\,GiB default compressed BAM file with 48\,GiB of memory did not produce a temporary file. \\
    The speedup of zlib on compression level 6 increases with every higher number of threads. In contrast, faster compression libraries such as igzip show a less substantial increase in speedup. When raising the number of threads from 8 to 16, their speedup decreases slightly.}
    \label{fig:bgzfSngleCoreSpeedup}
\end{figure}
This suggests that beyond a certain number of threads, compression, which is fully parallelized, is no longer the limiting factor. This is supported by the observation that the relative time \sort spends on decompression, sorting and merging together with compression converges to a similar ratio for all compression libraries and tested compression levels. \\
In the following, decompression stands for the time from starting \sort until starting the parallel sorting, sorting for the timespan from starting to ending the sorting and compression the timespan from end of sorting to the end of the \sort process. Having multiple threads available, \sort splits the BAM records in memory and sorts them in parallel. After the sorting, \sort merges the lists of sorted BAM records from the threads, writing each record immediately after merging it (buffered and with compression). Therefore, in the following, compression time includes merging. \\
On 16 threads, all compression libraries on their tested levels used 21\,\% to 31\,\% of their computation time for decompression, 6\,\% to 9\,\% for sorting and 60\,\% to 71\,\% for compression. Here, the faster compression methods use more of their computation time for decompression than slower methods like zlib on compression level 6. However, on a single thread, the faster libraries use in comparison to the slower libraries much more, around 50\,\%, of their time for decompression, while zlib on level 6 only uses 9\,\% of its computation time for decompression. On two threads, the percentage of the computation time the compression methods with a higher single-thread speedup spend on compression decreases further, but grows again with more used threads. Throughout the increasing number of threads, the proportion of compression compared to decompression increases with the faster libraries. In contrast, for the compression methods with less single-thread speedup (compared to zlib on compression level 6), the relative computation time spend on compressing decreases gradually up to around 70\,\% on 16 cores. This is illustrated in Figure \ref{fig:relative7BGZF}.\\
\afterpage{
    \begin{figure}[]
            \import{figures/}{relative7BGZFtimes.pgf}
        \caption{Comparison of the relative runtimes of decompression, sorting, and compression through 7BGZF compression libraries. Decompression: walltime from program startup of \sort until start of the parallel sorting step. Sorting: time spent sorting the DNA-Reads (?). Compression the timespan from end of sorting to the end of the \sort process. Therefore, merging is a part of the compression time as it is done concurrent. While the compression methods with a higher single-thread speedup compared to the default zlib compression on level 6 use a lower percentage of their computation time on compression on a single thread, the usages converge to a similar ratio on 16 threads. Sorting the 2.3\,GiB default compressed BAM file with 48\,GiB of memory did not produce a temporary file.}
        \label{fig:relative7BGZF}
    \end{figure}
    \clearpage
}
Therefore, faster compression seems not to scale as well as decompression. This is most likely due to the merging, which here is part of the compression time. Therefore, for larger numbers of threads, neither compression nor decompression seem to limit the sorting process, but the merging of temporary files. However, this could also shift if \sort produces temporary files. In this case, the merging would for every merged BAM record write and compress it and read and decompress a new one. Although compression and decompression are buffered and not for every read respective written record a new block has to be decompressed respective compressed, overlaps between compression and decompression are likely in this case. In case of in memory sorting, which is analyzed above, compression and decompression at the same time do not exist. 


Besides compression, decompression speed should be taken into account for the full picture. Here, differences between decompressing files compressed by 7BGZF compression libraries are within a range of 15\,\% around the average decompression time for compressed files. This holds for both decompression libraries zlib and libdeflate.
\begin{figure}
        \import{figures/}{decomp.pgf}
    \caption{Decompression speed by SAMtools using zlib compared to using libdeflate for decompression. Measured as execution time of SAMtools \texttt{view} with \texttt{-u} flag on a single core, piped to \texttt{/dev/null}. The test file is a 104\,GB uncompressed BAM file, compressed by \sort using the 7BGZF settings shown on the x-axis. HTSlib was build with \texttt{--with-libdeflate} for decompression utilizing libdeflate respective \texttt{--without-libdeflate} for the decompression utilizing zlib.\\
    Decompressing with Libdeflate takes about half the time as with zlib. The trend is for smaller files to decompress faster than larger ones.}
    \label{fig:decomp}
\end{figure}
The fastest decompression of compressed files varies depending on the decompression library used. Nevertheless, there is a trend indicating that files with smaller sizes, thus higher compression rates, are decompressed faster, as shown in Figure \ref{fig:decomp}. 
Additionally, it is evident that libdeflate is substantially faster than zlib for decompression. This still holds for the uncompressed file ("uncomp" in Figure \ref{fig:decomp}). This is due to a checksum calculation. For each read block, HTSlib calculates a crc32 checksum. Looking at SAMtools \texttt{view} in a profiler, the implementation for the crc32 checksum in libdeflate turns out to be about 10 times faster. \\

In conclusion, igzip, slz as well as zlib-ng and libdeflate on compression level 1 are very fast and suitable for temporary files and files to be used soon. On compression level 6, zlib-ng and libdeflate are good default values and provide a trade-off between file size and computation time. Zopfli offers very good compression, but the huge increase in computation time makes using high compression levels of zlib-ng and libdeflate more preferable in most settings for \sort.

\subsection{Recommendation}
To increase compression and decompression speed, the amount of threads \sort sort uses should be increased. This can be done using the "\texttt{-@}" parameter. Threads are also used for parallel sorting of in memory blocks of read BAM records. This further speeds up the process. The optimal number of threads to use depends on the compression settings. If \sort uses the default zlib implementation with the default compression level 6, using up to 16 threads is reasonable. For faster compression settings, using up to 8 cores is optimal. \\
For compression levels, it comes down to what the purpose of the sorted data is when deciding the level to be used. 
If the file should be archived, setting the level to 9 for maximal compression is an option. However, most of the time, sorting is a step in a larger pipeline, and the data is read and processed further soon. \\
To speed this up, I recommend to use compression level 0 ("\texttt{-l 0}" which is equal to "\texttt{-u}") if the data is not written directly to  disc or transferred over network with limited throughput. \\
If the data is written to disk directly or transferred via network, it comes down to the IO conditions which level to choose (see next section). In most cases, compression level 1 ("\texttt{-l 1}") is a good starting point.\\
When it comes to the zlib implementation to use, I recommend libdeflate. While not being the very fastest implementation, it is already supported by SAMtools. This means the stability is much higher as its usage is tested with SAMtools. Also, the faster crc32 implementation leads to performance improvements. At last, installing and using libdeflate is simpler than using \texttt{LD\_PRELOAD} or changing the environment for every user. After installation, the user can use SAMtools exactly as he has done before without libdeflate, but with better performance. \\
Users, concerned about output sizes but prioritizing a low execution time, can consider using igzip on compression level 1 via 7BGZF. Still, they should compile HTSlib with libdeflate support. 

\subsection{Evaluation}
\begin{figure}[t]
        \import{figures/}{speedupComps.pgf}
    \caption{Speedup of \sort after changing compression parameters. Reference is \sort using HTSlib with default zlib compression. Libdeflate is used via the HTSlib integration, igzip via 7BGZF. The input file is a 23.6\,GB unsorted BAM file with default zlib compression and a total of 48\,GiB memory. The output is piped to \texttt{/dev/null} to minimize IO impacts. Still, a temporary files is written to disk. \sort compresses the 23.6\,GB compressed input file to 21,1\,GB using zlib (the reference), 20.5\,GB using libdeflate on level 6, 22.6\,GB using libdeflate on level 1 and 31.5\,GB using igzip on level 1 (in both settings). \\
    For one to four threads \sort uses, the speedup with using faster compression is up to 5. However, with more threads available, the speedup decreases to approximately 1.5 for all compression methods.}
    \label{fig:speedupCompression}
\end{figure}
\begin{figure}[!h]
        \import{figures/}{speedupFinalCompsSingleCore.pgf}
    \caption{Speedup of \sort after changing compression parameters. The reference is the respective single-core performance (strong scaling). The default zlib implementation benefits most from a higher number of threads. The faster the compression speed, the less benefits come from using a higher number of threads. The underlying data is the data from Figure \ref{fig:speedupCompression}.}
    \label{fig:speedupCompression}
\end{figure}
Using igzip on compression level 1 via BGZF together with libdeflate for decompression leads to a speedup of up to 5 for single core sorting. This remains valid even if \sort generates a temporary file due to insufficient memory relative to the input file size. \\
Using the libdeflate integration of HTSlib we archive a speedup of 2 for the default compression level. Lowering the compression level of libdeflate to 1, we archive a speedup of 3.2 on 1 to 4 cores (see Figure \ref{fig:speedupCompression}). \\
Zlib on default compression level, as well as libdeflate on default compression level, profit from utilizing up to 16 cores (see Figure \ref{fig:speedupCompression}). Libdeflate on compression level one and igzip via 7BGZF profit the most from the increase to 8 threads. Still, changing from 8 to 16 used threads, they are  slightly faster (around 4\,\%), contrary to the previous results, where for igzip the execution time increased at changing from 8 to 16 used threads (see Figure \ref{fig:bgzfSngleCoreSpeedup}). \\
In conclusion, to reduce computation time of the compression done by \sort, the user can lower the compression level and choose a different zlib implementation. Libdeflate emerged to provide higher compression with lower computation time in both compression and decompression. Other libraries like igzip offer faster compression than libdeflate, but are currently not supported by HTSlib. Nevertheless, with certain restrictions, they can still be used via 7BGZF.


\subsection{Future Work}
Intel's igzip performs even better than libdeflate. Although this comes with the downside of larger files, implementing igzip support in HTSlib would enable even faster compression for temporary files. E.g., a mapping could be used mapping compression level 1 and 2 to compression level 1 and 3 of igzip and the higher levels to libdeflate levels. \\
For improving 7BGZF, a differentiation between output files and temporary files could be implemented.\\
Also, the reduced speedup at using faster compression libraries on a larger number of threads could be investigated further. E.g., the merging process could be removed to see if it limits the execution time of \sort.