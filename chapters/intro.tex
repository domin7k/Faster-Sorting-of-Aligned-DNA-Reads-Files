\section{Introduction}
To enable faster downstream analysis, aligned DNA-Read Files are sorted.

\subsection{SAMtools}
SAMtools \cite{12ySamtools} is a collection of tools to work on alignment data. It relies on the co-developed HTSlib \cite{bonfield_htslib_2021} for reading and writing information files, e.g. BAM files. SAMtools offers functionality for different operations on alignment data, such as format conversion, statistics, variant calling and many more, including the sorting, which is the focus here.

\subsection{SAM and BAM files}
A Sequence Alignment/Map (SAM) as specified by Li et al. \cite{samformat} is used to store the alignment of sequences against reference sequences. It consists of a Header Section and an Alignment Section. The Header Section contains meta information such as the format version or the sorting of the content and a dictionary of the reference sequences, whereas the Alignment Section contains aligned segments with alignment information and meta information such as the read quality. Here, a segment is a continuous sequence or subsequence of a raw DNA read. \\
The alignment information mainly consists of the ID of the reference sequence the alignment is mapped to, the position where the alignment starts in the reference sequence and a CIGAR string providing the alignment at this position. The CIGAR String as specified in \cite{samformat} lists i.a. sequential matches, mismatches, insertions and deletions and therefore represents the alignment of the corresponding segment and its reference sequence. \\
A BAM file is the binary representation of a SAM file. The main differences are the usage of a 4-bit encoding for the sequences, 3-bit for CIGAR Symbols and a 0-based instead of 1-based coordinate system for the position. \\
Furthermore, a BAM file is per default \textit{BGZF} compressed.

\subsection{BGZF Compression}
BGZF is a lossless compression method proposed at the same time as the BAM format. Widely used compression methods like GZIP compress a file from the beginning to the end in one piece. This has the advantage that matching pieces of the file can be found over a larger span. However, to decrypt such a compressed file, it also needs to be read from the beginning and, depending on the compression method, decompressed at least until the point of interest. Since alignment data can produce very large files but not all of their regions are needed for every use case, it is beneficial to enable some form of random access. To archive this, BGZF utilizes \textit{GZIP} \cite{gzip} and the \textit{DEFLATE} algorithm \cite{deflate} by Phil Katz to compress large files into blocks of less than 64KB size (compressed and uncompressed). These blocks are the concatenated. Thus, fast random access using index files is possible. In an index file, the position of a piece of information is stored in a 64-bit integer. It consists of a 48-bit unsigned integer \textit{coffset} indicating the number of the compressed block and a 16-bit unsigned integer \textit{uoffset} describing the position in the uncompressed block. \\
Besides this, the BGZF format also provides compatibility with GZIP. As GZIP allows this combination of multiple compressed files to one file, a BGZF-compressed file can be decompressed by any standard GZIP implementation. This can be practical if for example a SAM file that can be viewed with any text editor is compressed using BGZF. \\
Like GZIP, BGZF supports compression levels ranging from 1 (fastest but worst) to 9 (slowest but best) which are basically the compression levels used for the underlying GZIP compression. The compression levels affect the size of the compressed files as shown in Figure \ref{fig:compSizes}.
\begin{figure}
        \import{figures/}{compSizes.pgf}
    \caption{Comparison of the size of BGZF compressed files on all compression levels, exemplified using a 10.4GiB unsorted BAM file.}
    \label{fig:compSizes}
\end{figure}
The speed of the compression depends mainly on the compression level, the GZIP-implementation and the number of used threads (see Figure \ref{fig:compSpeed}).  
\begin{figure}
        \import{figures/}{compSpeed.pgf}
    \caption{Comparison of the output rate of HTSlib's \texttt{bgzip} which uses BGZF to compress A 10.4GiB unsorted BAM file. For reference, compression level 0 is plotted in the smaller inset plot.}
    \label{fig:compSpeed}
\end{figure}
To measure the compression speed while minimizing the influence of the sort and merge processes, the speed of HTSlib's \texttt{bgzip} can be measured. This is a tool using BGZF to compress arbitrary files. As its inner mechanisms are exactly the same as the compression part of SAMtools' \texttt{sort} tool (use the same underlying HTSlib methods), pipelining \texttt{bgzip}'s output to \texttt{/dev/null} gives an estimation of the computation time needed only for compression at using SAMtools \texttt{sort}. This still holds for compression level 0. At this level, the input is not compressed, but directly written to the output.

