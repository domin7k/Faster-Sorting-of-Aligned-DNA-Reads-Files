\subsection{Prerequisites}
The process of sorting alternates, depending on some internal Constants and command-line-arguments: \\
We only focus on sorting by the order of the reference, then position and then the REVERSE flag which indicates, if the sequence is aligned forward or backward to the reference. This order is the one used by SAMtools per default, although other sorting criteria e.g. tags or the read name are possible.\\
The maximum amount of memory used to sorting is calculated by the amount of memory the user specifies via the \texttt{-m} option multiplied by the (via \texttt{-@} option) assigned number of threads. Here, we refer to the total amount as \texttt{max\_mem}. \\
The in- and output formats are per default inferred from the file names. \\
SAMtools \texttt{sort} passes its output to standard output if no output file is specified. In this case, the output format is set to BAM.
The maximum number of temporary files is hard-coded as 64 in a constant named \texttt{MAX\_TMP\_FILES}. \\
The gzip compression level for temporary files is set to 1, while the compression level of the result file can be changed via the \texttt{-l} parameter. It defaults to the default compression level used by the library that implements the compression, usually 6, but can be set to a number between 0 (no compression) and 9 (highest and slowest compression).

\subsection{Sorting} \label{sorting}

SAMtools performs an external sort process using temporary files that are merged in the end. The sorting process flow is represented by the flowchart in Figure \ref{fig:flow}.
\begin{figure}[ht]
    \begin{adjustbox}{width=\linewidth}
        \input{figures/flowchart}
    \end{adjustbox}
    \caption{Flow chart showing the current process of sorting, especially the choosing of files to be merged. The list of files is a 0-based list of their names. In the beginning it is empty, after BAM records are read the second time, there is a single record at position 0 and \#Files is 1.}
    \label{fig:flow}
\end{figure}
The sorting starts by sequentially reading BAM records from the input file using HTSlib for parallel decompression. Once the memory limit given by \texttt{max\_mem} is exceeded, these records are split into as many blocks as threads are specified and afterward sorted in parallel. \\
Then, the merge is performed. In the merge, all the sorted in-memory files are written to a single sorted temporary BAM file. In Addition, some of the previously created temporary files are added: The algorithm distinguishes between small files and big files. Small files are files generated by merging one set of in memory blocks. If the number of small files is greater than half of the maximum allowed number of temporary files, all the small files are merged (and afterward deleted). The result of a merge of in-memory and temporary files is a big file. If the total number of files exceeds the limit for temporary files, all temporary files including big files are included in the merge (and afterward deleted). The resulting file is also counted as a big file, despite possibly being much larger than other big files generated by merging only small files. However, as the first merging of big files occurs at the 1120th temporary file\footnote{This number is the result of adding $33 \cdot 33$ temporary files already merged into big files to $31$ small files. Here we have to square $33$, as $32$ small files can exist, and the $33$rd file is the big file which the result of a merge, but not counted among the small files. If $32$ big files exist, there is still space for 32 small files, and they are merged to a $33$rd big file, leaving only space for $31$ small files in the next merging process.}, this is only relevant for combinations of very big files and little memory. \\
In general, the temporary files on the disc can be put into three categories: small files being at most as big as the sorted in-memory blocks together, big files being at most as big as half of the maximum number of allowed temporary files times the maximum size for small files and one big file growing depending on the ratio of allocated memory to the size of the input file possibly to much bigger size than the other big files. \\
After the merge, the algorithm repeats the previous steps until the end of the input file is reached. As the last step, the remaining in-memory BAM records are sorted and merged together with all temporary files and written to the output file.