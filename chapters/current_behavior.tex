\subsection{Prerequisites}
The process of sorting alternates depending on some internal Constants and command-line-arguments: 
We focus on sorting by the ID of the reference a DNA-Read is aligned to, then the position where the alignment on the reference starts, and then the REVERSE flag which indicates, if the sequence is aligned forward or backward to the reference. This order is used by SAMtools per default, although other sorting criteria e.g., tags or the read name are possible.

The maximum amount of memory \sort utilizes for storing BAM records during the sorting process is calculated by the amount of memory the user specifies via the \texttt{-m} option multiplied by the (via \texttt{-@} option) assigned number of threads. We refer to the total amount as \texttt{max\_mem}. 

Users can specify the number of threads to use for \sort by the \texttt{-@} parameter. If it is set to 1, the operation is single threaded. If the user sets it to a number greater than one, \sort uses the specified number of threads in addition to the main thread.

\sort infers the in- and output formats from the input and output file names the user specifies. SAMtools \texttt{sort} passes the sorted aligned DNA-Read files it outputs as BAM file to standard output if no output file is specified.

The maximum number of temporary files is hard-coded as 64 in a constant named \texttt{MAX\_TMP\_FILES}. 

Temporary files utilized in the sorting process are compressed with compression level 1. The compression level of the result file defaults to the default compression level used by the library that implements the compression. Usually this is compression level 6. The user can change the compression level of the output file via the \texttt{-l} parameter and set it to a number between 0 (no compression) and 9 (highest and slowest compression).

\subsection{Sorting} \label{sorting}

SAMtools performs an external memory sort utilizing temporary files that are merged in the end.
The sorting starts by sequentially reading BAM records from the input BAM file, or stream using HTSlib for parallel decompression. Once BAM records, which contain alignment information for DNA-Reads, exceed the memory limit given by \texttt{max\_mem}, \sort splits these BAM records into as many in-memory vectors as threads are specified and afterward sorts them in parallel. For sorting, each thread used for sorting (on multithreading, each thread except the main thread) performs a radix sort.

Then, \sort performs a heap based merge. In the merge, \sort keeps a binary heap containing the smallest entry from each file, and in-memory vector that should be merged. Each of these entries is a BAM record with the lowest order from the respective file or in-memory vector, together with a reference to the file or in-memory vector. The merge works by outputting the entry of the heap with the lowest order, inserting the next entry from the file or in-memory vector this entry came from into the heap, and adjusting the heap using the \textit{sift-down} algorithm~\cite{bojesen_performance_1999}. 

In the merge, \sort writes all the sorted in-memory vectors of BAM records to a single sorted temporary BAM file. In addition, \sort includes some of the previously created temporary files into the merge: The algorithm distinguishes between \textit{small files} and \textit{big files}. Small files are files generated by merging the sorted in-memory vectors of BAM records resulting from sorting them in parallel. If the number of small files is greater than half of the maximum allowed number of temporary files, \sort includes all small files it generated before into the merge (and deletes them afterward). The result of a merge of in-memory vectors of BAM records and temporary files is a big file. If the total number of files (small files and big files) exceeds the limit for temporary files, all previously generated temporary files including big files are included in the merge and deleted afterward. The resulting file is also counted as a big file, despite possibly being much larger than other big files generated by merging only small files. However, as the first merging of big files occurs at the 1120th temporary file\footnote{\label{limitReaching}This number is the result of adding $33 \cdot 33$ temporary files already merged into big files to $31$ small files. Here we have to square $33$, as $32$ small files can exist, and the $33$rd file is the big file which the result of a merge, but not counted among the small files. If $32$ big files exist, there is still space for $32$ small files, and they are merged to a $33$rd big file, leaving only space for $31$ small files in the next merging process.}, this is only relevant for combinations of enormous files and little memory. 

In general, temporary files can be put into three categories: small files being at most as big as the sorted in-memory vectors of BAM records together, big files being at most as big as half of the maximum number of allowed temporary files times the maximum size for small files and one big file growing depending on the ratio of allocated memory to the size of the input file possibly to much larger size than the other big files.

After the merge, the algorithm repeats the previous steps from reading the input to merging, until \sort reaches the end of the input file or stream. As a last step, \sort sorts the remaining in-memory BAM records and merges them together with all temporary files into the output file.
The sorting process flow is represented by the flowchart in Figure \ref{fig:flow}.

\begin{figure}[ht]
    \begin{adjustbox}{width=\linewidth}
        \input{figures/flowchart}
    \end{adjustbox}
    \caption{Flow chart showing the current process of sorting, especially the choosing of files to be merged. The list of files is a 0-indexed list of their names. In the beginning it is empty, after BAM records are read the second time, there is a single record at position 0 and \#Files is 1.}
    \label{fig:flow}
\end{figure}