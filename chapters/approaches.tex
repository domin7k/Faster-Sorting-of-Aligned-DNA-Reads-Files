\section{Failed Approaches}
\subsection{Storing Pointers}
The initial idea to speed up the sorting process consisted of the following steps: 
\begin{enumerate}
    \item Read once through the whole input file. For every BAM record containing alignment information on a DNA-Read, store a pointer to the location of the BAM record on the disk together with the attributes needed for sorting, the ID of the reference sequence, the DNA-Read is aligned to, the starting position of the alignment on this reference sequence and the REVERSE flag.
    \item Sort the resulting list based on the extracted attributes. Due to their significantly smaller memory footprint compared to complete BAM records, sorting these attributes can be efficiently performed in-memory.
    \item Iterate over the resulting sorted list. For every entry, read the referenced BAM record from disk using random reads and write it sequentially into the output file.
\end{enumerate}

Although this method eliminates the need to write intermediate files, which currently consumes a substantial portion of the time needed for sorting (\Cref{tempfiles}), it has some drawbacks: 

BAM files are binary compressed representations of SAM files containing alignment information of DNA-Reads. While compression is beneficial to store and transfer the huge amounts (up to multiple terabytes per file) of data an aligned DNA-Read file can consist of, it makes random access a lot harder. Usually, a compressed file has to be decompressed from start to at least the position the user is interested in. To address this, BAM files are compressed in the BGZF file format. As a file in the BGZF file format consists of small blocks (less than 64\,KB uncompressed) compressed individually in the  DEFLATE format, for a random read only the number of the block the BAM record for the aligned DNA-Read is in, together with an offset into the compressed block is necessary. 

However, this method is not suitable for accessing every single record in a file in random order:
As mentioned before, the DEFLATE compressed blocks in a BGZF file typically have sizes of 64\,KB of uncompressed data. Within our main test file, BAM records had on average a size of about 250 bytes. Therefore, a DEFLATE compressed block on average contains 256 BAM records. To extract every record in random order, the block has to be decompressed 256 times on average to halfway. Moreover, if the input file is very large in comparison to the available memory, caching  the uncompressed blocks is not feasible. 

We can now approximate the compression and decompression operations per BAM record at the current state of \sort, compared to this approach:\footnote{For simplicity, we ignore possible caching of uncompressed blocks in the approach using random reads.} Currently, \sort decompresses the input file once, accounting for one decompression operation for every 265 BAM records. Then, the record is written to the temporary file, resulting in one compression operation for every 256 records. In the final merge, \sort reads the temporary file again (one compression operation per 256 records) and writes the output file (again, one compression operation for every 256 records). Input and output decompression and compression are necessary for both approaches, therefore they account for the same amount of compression and decompression operations in both approaches.

The approach using random reads, however, does not use compression operations in between reading the input file and writing the output file, but for every BAM record on average one compression operation on half a block, accumulating to around 128 decompression operations on whole blocks per 256 BAM records. Therefore, for the approach using random reads to be faster than the current behavior of \sort, compression of a single block (together with writing) would have to be 127 times slower than the combination of reading and decompressing a compressed block. As this seems unlikely on most systems, no improvement is expected from this approach. 

In addition, having to read the file two times breaks the ability of SAMtools sort to work on a stream. As this is a core feature of SAMtools, breaking it should be avoided.

\subsection{Removing Compression of Temporary Files}
Our first measurements showed, that \sort spends most of its computation time for compression, even if it outputs uncompressed aligned DNA-Read files. Based on this observation, our initial assumption was that removing the compression of the temporary files would reduce the runtime of \sort. However, experimenting with faster compression libraries and utilizing a larger amount of CPUs, removing the compression of files turned out to be slower than keeping it on a low level (\Cref{ioComp}). Therefore, we decided not to change the compression \sort applies to temporary files. 