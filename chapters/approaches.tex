\section{Approaches}
\subsection{Storing Pointers}
The initial idea to speed up the sorting process consisted of the following steps: 
\begin{enumerate}
    \item Read once through the whole input file. For every BAM record, store a pointer to the location of the record on the disk together with the attributes needed for sorting, i.e. the reference ID, the position and the REVERSE flag.
    \item Sort the resulting list. As the attributes extracted are much smaller than whole records, this should be possible in memory.
    \item Iterate over the sorted list. For every entry, read the BAM record placed at the location the pointer points to using random reads and write it sequentially into the output file.
\end{enumerate}

Although this method eliminates the need to write intermediate files, which currently consumes a substantial portion of the time needed for sorting, it has some drawbacks: \\
BAM files are binary compressed representations of SAM files. While the compression usually is beneficial to store and transfer the huge amounts of data a SAM file can consist of, it makes random access a lot harder. Usually, a compressed file has to be decompressed from start to at least the position the user is interested in. Especially with the used compression and decompression algorithms, DEFLATE and INFLATE both being streams, every random read would require decompressing the compressed file from the beginning. To solve this, BAM files are compressed using BGZF. As only small blocks are compressed by DEFLATE, for a random read only the number of the block the read is in, together with an offset into the compressed block is needed. \\
However, this method is not suitable for accessing every single record in a file in random order: \\
Blocks typically have sizes of 64KB of uncompressed data. Within our main test file, BAM records had on average a size of about 250 bytes. Therefore, a block on average contains 256 BAM records. To extract every record in random order, the block has to be decompressed 256 times on average to halfway. To make things worse, if the input file is very large in comparison to the available memory, caching  the uncompressed blocks gets less effective. \\
If no merging of temporary files has to be performed, we can now calculate the deflate and inflate operations per BAM record at the current state of SAMtools sort compared to this approach: Currently, the Input file is decompressed once accounting for one INFLATE call for every 265 BAM records. Then, the record is written to the temporary file, resulting in one DEFLATE call for every 256 records. After some time, the temporary file is read again (one INFLATE call per 256 records) and the output file is written (again, one DEFLATE call per 256 records). As the input and the output decompression and compression are necessary for both approaches, they can be ignored. \\
The approach using random reads, however, does not use any DEFLATE call in between input and output, but for every BAM record on average one execution INFLATE on half of a block, accumulating to around 128 INFLATE calls on whole blocks per 256 BAM records. Therefore, to speed up the operation, a single DEFLATE execution (together with writing) would have to be 127 times slower than the combination of reading and INFLATE. As this seems unlikely on most systems, no improvement is expected from this approach. \\
In addition, having to read the file two times breaks the ability of SAMtools sort to work on a stream. As this is a core feature of SAMtools, breaking it should be avoided.

\subsection{Adjust Compression of Temporary Files}
The first measurements showed, that the biggest part of computation time is used for compression even if the compression of the output format is removed. Based on this observation, it seemed obvious, that removing the compression of the temporary files would speed up the process. More experiments on another computer seemed to confirm this hypothesis. However, in final experiments above a threshold of used threads, removing the compression of files turned out to be slower than keeping it on a low level. Where does this change of directions come from? \\
If more or less compression is faster depends on the proportion between compression speed and write speed. The compression speed is mainly influenced by the gzip compression level, the number of available cores, their speed and the implementation of the DEFLATE algorithm. As the output consists of many independly compressed blocks, this can be done in parallel. Thus, increasing the number of threads substantially increases the compression speed if enough physical cores are available. Lowering the compression level also results in a speedup, but as the compression level of temporary files is already 1 the only way to reduce it further would be to disable compression (level 0). Of course, changing to a faster compression library also increases the compression speed. \\
If the write speed the disk offers is less than the compression output, there is an IO bottleneck. To detect this, we can have a look on \textit{IOWait} time. IOWait time is the amount of time, the CPU spends in IDLE because it has to wait for IO Operations. This is measured for all cores together. 
