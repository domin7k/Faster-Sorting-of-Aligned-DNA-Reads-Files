\section{Approaches}
\section{Storing Pointers}
The initial idea to speed up the sorting process consisted of the following steps: 
\begin{enumerate}
    \item Read once through the whole input file. For every BAM record, store a pointer to the location of the record on the disk together with the attributes needed for sorting, i.e. the reference ID, the position and the REVERSE flag.
    \item Sort the resulting list. As the attributes extracted are much smaller than whole records, this should be possible in memory.
    \item Iterate over the sorted list. For every entry, read the BAM record placed at the location the pointer points to using random reads and write it sequentially into the output file.
\end{enumerate}

Although this method eliminates the need to write intermediate files, which currently consumes a significant portion of the time needed for sorting, it has some drawbacks: \\
BAM files are binary compressed representations of SAM files. While the compression usually is beneficial to store and transfer the huge amounts of data a SAM file can consist of, it makes random access a lot harder. Usually, a compressed file has to be decompressed from start to at least the position the user is interested in. Especially with the used compression and decompression algorithms, DEFLATE and INFLATE both being streams, every random read would require decompressing the compressed file from the beginning. To solve this, BAM files are compressed using BGZF. As only small blocks are compressed by DEFLATE, for a random read only the number of the block the read is in, together with an offset into the compressed block is needed. \\
However, this method is not suitable for accessing every single record in a file in random order: \\
Blocks typically have sizes of 64KB of uncompressed data. Within our main test file, BAM records had on average a size of about 250 bytes. Therefore, a block on average contains 256 BAM records. To extract every record in random order, the block has to be decompressed 256 times on average to halfway. To make things worse, if the input file is very large in comparison to the available memory, caching  the uncompressed blocks gets less effective. \\
If no merging of temporary files has to be performed, we can now calculate the deflate and inflate operations per BAM record at the current state of SAMtools sort compared to this approach: Currently, the Input file is decompressed once accounting for one INFLATE call for every 265 BAM records. Then, the record is written to the temporary file, resulting in one DEFLATE call for every 256 records. After some time, the temporary file is read again (one INFLATE call per 256 records) and the output file is written (again, one DEFLATE call per 256 records). As the input and the output decompression and compression are necessary for both approaches, they can be ignored. \\
The approach using random reads, however, does not use any DEFLATE call in between input and output, but for every BAM record on average one execution INFLATE on half of a block, accumulating to around 128 INFLATE calls on whole blocks per 256 BAM records. Therefore, to speed up the operation, a single DEFLATE execution (together with writing) would have to be 127 times slower than the combination of reading and INFLATE. As this seems unlikely on most systems, no improvement is expected from this approach. \\
In addition, having to read the file two times breaks the ability of SAMtools sort to work on a stream. As this is a core feature of SAMtools, breaking it should be avoided.